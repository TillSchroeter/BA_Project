{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "521b9f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%-------------------------------------------------------------------------------------------\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt, iirnotch\n",
    "\n",
    "import functions_IMU\n",
    "import functions_generell\n",
    "import functions_PRESSURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c8897da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daten für ID_1_Dabisch_Samuel geladen\n",
      "Daten für ID_2_Pohl_Jannis geladen\n",
      "Daten für ID_3_Kleber_Christian geladen\n",
      "Daten für ID_4_Schröter_Till geladen\n",
      "Daten für ID_5_Zaschke_Lenard geladen\n",
      "Daten für ID_6_Petroll_Finn geladen\n",
      "Daten für ID_7_Gruber_Julius geladen\n",
      "\n",
      "Alle Daten geladen! 7 Teilnehmer erfolgreich eingelesen.\n"
     ]
    }
   ],
   "source": [
    "#%% Daten einlesen für alle Teilnehmer-------------------------------------------------------------------------------------------\n",
    "# liste der Teilneher\n",
    "participants = ['ID_1_Dabisch_Samuel', 'ID_2_Pohl_Jannis', 'ID_3_Kleber_Christian',\n",
    "                'ID_4_Schröter_Till', 'ID_5_Zaschke_Lenard', 'ID_6_Petroll_Finn', 'ID_7_Gruber_Julius']\n",
    "# bedingunegn der Messungen\n",
    "measurements = ['REAL_1', 'REAL_2', 'VR_1', 'VR_2']\n",
    "\n",
    "# Alle Daten der Teilnehmer einlesen und verarbeiten\n",
    "all_data = {}\n",
    "\n",
    "for participant in participants:\n",
    "    folder_path = os.path.join('data_final', participant)\n",
    "    \n",
    "    # Lade die 6 CSV-Dateien für den Teilnehmer\n",
    "    real_1, real_2, vr_1, vr_2, mvc_beine, mvc_hals = functions_generell.load_csvs(folder_path)\n",
    "    \n",
    "    # Speichere die DataFrames im Dictionary\n",
    "    all_data[participant] = {\n",
    "        'REAL_1': real_1,\n",
    "        'REAL_2': real_2,\n",
    "        'VR_1': vr_1,\n",
    "        'VR_2': vr_2,\n",
    "        'MVC_Beine': mvc_beine,\n",
    "        'MVC_Hals': mvc_hals\n",
    "    }\n",
    "    \n",
    "    print(f\"Daten für {participant} geladen\")\n",
    "\n",
    "print(f\"\\nAlle Daten geladen! {len(all_data)} Teilnehmer erfolgreich eingelesen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55e4163",
   "metadata": {},
   "source": [
    "## hier werden der takeoff zeitpunkt anhand des Kniegelenks bestimmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47bcd0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verarbeite ID_1_Dabisch_Samuel...\n",
      "  REAL_1: 6 Sprünge gefunden\n",
      "  REAL_2: 6 Sprünge gefunden\n",
      "  VR_1: 6 Sprünge gefunden\n",
      "  VR_2: 6 Sprünge gefunden\n",
      "  ✓ Gesamt: 24 Sprünge\n",
      "  ✓ CSV gespeichert: jump_analysis_results_knee\\ID_1_Dabisch_Samuel_jumps.csv\n",
      "\n",
      "Verarbeite ID_2_Pohl_Jannis...\n",
      "  REAL_1: 6 Sprünge gefunden\n",
      "  REAL_2: 5 Sprünge gefunden\n",
      "  VR_1: 6 Sprünge gefunden\n",
      "  VR_2: 1 Sprünge gefunden\n",
      "  ✓ Gesamt: 18 Sprünge\n",
      "  ✓ CSV gespeichert: jump_analysis_results_knee\\ID_2_Pohl_Jannis_jumps.csv\n",
      "\n",
      "Verarbeite ID_3_Kleber_Christian...\n",
      "  REAL_1: 6 Sprünge gefunden\n",
      "  REAL_2: 6 Sprünge gefunden\n",
      "  VR_1: 6 Sprünge gefunden\n",
      "  VR_2: 6 Sprünge gefunden\n",
      "  ✓ Gesamt: 24 Sprünge\n",
      "  ✓ CSV gespeichert: jump_analysis_results_knee\\ID_3_Kleber_Christian_jumps.csv\n",
      "\n",
      "Verarbeite ID_4_Schröter_Till...\n",
      "  REAL_1: 6 Sprünge gefunden\n",
      "  REAL_2: 6 Sprünge gefunden\n",
      "  VR_1: 5 Sprünge gefunden\n",
      "  VR_2: 6 Sprünge gefunden\n",
      "  ✓ Gesamt: 23 Sprünge\n",
      "  ✓ CSV gespeichert: jump_analysis_results_knee\\ID_4_Schröter_Till_jumps.csv\n",
      "\n",
      "Verarbeite ID_5_Zaschke_Lenard...\n",
      "  REAL_1: 6 Sprünge gefunden\n",
      "  REAL_2: 6 Sprünge gefunden\n",
      "  VR_1: 6 Sprünge gefunden\n",
      "  VR_2: 6 Sprünge gefunden\n",
      "  ✓ Gesamt: 24 Sprünge\n",
      "  ✓ CSV gespeichert: jump_analysis_results_knee\\ID_5_Zaschke_Lenard_jumps.csv\n",
      "\n",
      "Verarbeite ID_6_Petroll_Finn...\n",
      "  REAL_1: 6 Sprünge gefunden\n",
      "  REAL_2: 6 Sprünge gefunden\n",
      "  VR_1: 6 Sprünge gefunden\n",
      "  VR_2: 6 Sprünge gefunden\n",
      "  ✓ Gesamt: 24 Sprünge\n",
      "  ✓ CSV gespeichert: jump_analysis_results_knee\\ID_6_Petroll_Finn_jumps.csv\n",
      "\n",
      "Verarbeite ID_7_Gruber_Julius...\n",
      "  REAL_1: 6 Sprünge gefunden\n",
      "  REAL_2: 6 Sprünge gefunden\n",
      "  VR_1: 6 Sprünge gefunden\n",
      "  VR_2: 6 Sprünge gefunden\n",
      "  ✓ Gesamt: 24 Sprünge\n",
      "  ✓ CSV gespeichert: jump_analysis_results_knee\\ID_7_Gruber_Julius_jumps.csv\n",
      "\n",
      "================================================================================\n",
      "Verarbeitung abgeschlossen!\n",
      "Alle CSVs wurden im Ordner 'jump_analysis_results_knee' gespeichert.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "### takeofoff mit kniewinkel\n",
    "def identify_jumps_by_knee(df, angle_col='RT Knee Flexion (deg)', min_jump_distance_sec=5.0, threshold_angle=60, buffer=0.75, fs=2000):\n",
    "    \"\"\"\n",
    "    Identifiziert Sprünge sequentiell ohne Filterung:\n",
    "    Sucht nach dem Peak > 60° und nimmt das ALLERERSTE lokale Minimum danach als Takeoff.\n",
    "    \"\"\"\n",
    "    angle = df[angle_col].values\n",
    "    time = df['time'].values\n",
    "    \n",
    "    jumps_list = []\n",
    "    i = 0\n",
    "    pause_points = int(min_jump_distance_sec * fs)\n",
    "    search_window_points = int(1.5 * fs)\n",
    "\n",
    "    while i < len(angle):\n",
    "        # A. Suche nach dem nächsten Punkt über dem Schwellenwert (Ausholbewegung)\n",
    "        if angle[i] >= threshold_angle:\n",
    "            # 1. Den exakten Peak (maximale Beugung) im Umkreis finden\n",
    "            peak_search_end = min(i + int(0.5 * fs), len(angle))\n",
    "            peak_idx = i + np.argmax(angle[i:peak_search_end])\n",
    "            \n",
    "            # 2. Suche Takeoff: Das ERSTE lokale Minimum nach dem Peak\n",
    "            takeoff_idx = peak_idx # Fallback\n",
    "            \n",
    "            # Wir laufen vom Peak vorwärts und suchen den ersten Punkt, \n",
    "            # an dem die Kurve nicht mehr weiter fällt.\n",
    "            for j in range(peak_idx + 1, min(peak_idx + search_window_points, len(angle) - 1)):\n",
    "                if angle[j] <= angle[j-1] and angle[j] <= angle[j+1]:\n",
    "                    takeoff_idx = j\n",
    "                    break\n",
    "            else:\n",
    "                # Falls kein lokales Minimum gefunden wurde, Backup: Absolutes Minimum\n",
    "                search_end = min(peak_idx + search_window_points, len(angle))\n",
    "                takeoff_idx = peak_idx + np.argmin(angle[peak_idx:search_end])\n",
    "\n",
    "            # 3. Daten extrahieren\n",
    "            t_absprung = time[takeoff_idx]\n",
    "            start_analyse = max(0, t_absprung - buffer)\n",
    "            \n",
    "            jumps_list.append({\n",
    "                'sprung nr.': len(jumps_list) + 1,\n",
    "                'start_analyse': round(start_analyse, 4),\n",
    "                't_absprung': round(t_absprung, 4)\n",
    "            })\n",
    "            \n",
    "            # 4. Sperrfrist: 5 Sekunden nach dem Takeoff überspringen\n",
    "            i = takeoff_idx + pause_points\n",
    "            continue\n",
    "        \n",
    "        i += 1\n",
    "            \n",
    "    return jumps_list\n",
    "\n",
    "def visualize_knee_jumps(df, jumps_list, participant=\"\", measurement=\"\"):\n",
    "    \"\"\"\n",
    "    Visualisiert die Sprungerkennung basierend auf dem Kniewinkel.\n",
    "    Korrektur: alpha in tick_params entfernt, da nicht unterstützt.\n",
    "    \"\"\"\n",
    "    time = df['time'].values\n",
    "    # Sicherstellen, dass die Spalte exakt so heißt\n",
    "    knee_angle = df['RT Knee Flexion (deg)'].values\n",
    "    total_force = df['LT Force (N)'] + df['RT Force (N)']\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(15, 7))\n",
    "\n",
    "    # Primäre Achse: Kniewinkel\n",
    "    ax1.plot(time, knee_angle, color='blue', alpha=0.6, label='Kniewinkel (RT Flexion)')\n",
    "    ax1.set_ylabel('Kniewinkel (deg)', color='blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue') # alpha entfernt\n",
    "\n",
    "    # # Sekundäre Achse für die Kraft\n",
    "    # ax2 = ax1.twinx()\n",
    "    # ax2.plot(time, total_force, color='black', alpha=0.2, label='Gesamtkraft (N)')\n",
    "    # ax2.set_ylabel('Kraft (N)', color='gray') \n",
    "    # ax2.tick_params(axis='y', labelcolor='gray') # alpha entfernt\n",
    "\n",
    "    labeled_buffer = False\n",
    "    labeled_takeoff = False\n",
    "\n",
    "    for j in jumps_list:\n",
    "        # 1. Analyse-Fenster (Buffer bis Takeoff)\n",
    "        # Beachte: Die Keys müssen 'start_analyse' und 't_absprung' sein (wie in deiner Funktion)\n",
    "        ax1.axvspan(j['start_analyse'], j['t_absprung'], \n",
    "                    color='gray', alpha=0.15, \n",
    "                    label='Analyse-Fenster (Buffer)' if not labeled_buffer else \"\")\n",
    "        labeled_buffer = True\n",
    "        \n",
    "        # 2. Vertikale Linie für Takeoff\n",
    "        ax1.axvline(j['t_absprung'], color='red', linestyle='--', lw=2,\n",
    "                    label='Takeoff (Knie-Extremum)' if not labeled_takeoff else \"\")\n",
    "        labeled_takeoff = True\n",
    "        \n",
    "        # 3. Sprungnummer\n",
    "        ax1.text(j['t_absprung'], max(knee_angle) * 0.95, f\"S{j['sprung nr.']}\", \n",
    "                 color='red', fontweight='bold', ha='right')\n",
    "\n",
    "    plt.title(f\"Knie-basierte Sprungerkennung: {participant} - {measurement} ({len(jumps_list)} Sprünge)\")\n",
    "    ax1.set_xlabel(\"Zeit (s)\")\n",
    "    \n",
    "    # Legenden zusammenführen\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    # lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 , labels1 , loc='upper right')\n",
    "    \n",
    "    ax1.grid(True, alpha=0.2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "output_folder = 'jump_analysis_results_knee'\n",
    "\n",
    "# Erstelle Ausgabeverzeichnis, falls nicht vorhanden\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for participant in participants:\n",
    "    print(f\"\\nVerarbeite {participant}...\")\n",
    "    \n",
    "    all_jumps_data = []\n",
    "    \n",
    "    # Durchlaufe alle 4 Messungen\n",
    "    for measurement in measurements:\n",
    "        df = all_data[participant][measurement]\n",
    "        \n",
    "        # Identifiziere Sprünge in dieser Messung\n",
    "        jumps = identify_jumps_by_knee(df,  min_jump_distance_sec = 5, buffer=0.7)\n",
    "        #visualize_knee_jumps(df, jumps, participant, measurement)\n",
    "\n",
    "        # Füge Measurement-Info zu jedem Sprung hinzu\n",
    "        for jump in jumps:\n",
    "            jump['messung'] = measurement\n",
    "            all_jumps_data.append(jump)\n",
    "        \n",
    "        print(f\"  {measurement}: {len(jumps)} Sprünge gefunden\")\n",
    "    \n",
    "    # Erstelle DataFrame aus allen Sprüngen\n",
    "    jumps_df = pd.DataFrame(all_jumps_data)\n",
    "    \n",
    "    # Speichere als CSV\n",
    "    output_file = os.path.join(output_folder, f'{participant}_jumps.csv')\n",
    "    jumps_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"  ✓ Gesamt: {len(all_jumps_data)} Sprünge\")\n",
    "    print(f\"  ✓ CSV gespeichert: {output_file}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Verarbeitung abgeschlossen!\")\n",
    "print(f\"Alle CSVs wurden im Ordner '{output_folder}' gespeichert.\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331462b1",
   "metadata": {},
   "source": [
    "## Hier werden die Jump parameter (dauer, Höhe) anhand der Kraft daten bestimmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a58bec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verarbeite ID_1_Dabisch_Samuel...\n",
      "  REAL_1: 6 Sprünge gefunden\n",
      "  REAL_2: 6 Sprünge gefunden\n",
      "  VR_1: 6 Sprünge gefunden\n",
      "  VR_2: 6 Sprünge gefunden\n",
      "  ✓ Gesamt: 24 Sprünge\n",
      "  ✓ CSV gespeichert: jump_analysis_results_Pressure\\ID_1_Dabisch_Samuel_jumps.csv\n",
      "\n",
      "Verarbeite ID_2_Pohl_Jannis...\n",
      "  REAL_1: 6 Sprünge gefunden\n",
      "  REAL_2: 5 Sprünge gefunden\n",
      "  VR_1: 6 Sprünge gefunden\n",
      "  VR_2: 2 Sprünge gefunden\n",
      "  ✓ Gesamt: 19 Sprünge\n",
      "  ✓ CSV gespeichert: jump_analysis_results_Pressure\\ID_2_Pohl_Jannis_jumps.csv\n",
      "\n",
      "Verarbeite ID_3_Kleber_Christian...\n",
      "  REAL_1: 6 Sprünge gefunden\n",
      "  REAL_2: 6 Sprünge gefunden\n",
      "  VR_1: 6 Sprünge gefunden\n",
      "  VR_2: 6 Sprünge gefunden\n",
      "  ✓ Gesamt: 24 Sprünge\n",
      "  ✓ CSV gespeichert: jump_analysis_results_Pressure\\ID_3_Kleber_Christian_jumps.csv\n",
      "\n",
      "Verarbeite ID_4_Schröter_Till...\n",
      "  REAL_1: 6 Sprünge gefunden\n",
      "  REAL_2: 6 Sprünge gefunden\n",
      "  VR_1: 6 Sprünge gefunden\n",
      "  VR_2: 6 Sprünge gefunden\n",
      "  ✓ Gesamt: 24 Sprünge\n",
      "  ✓ CSV gespeichert: jump_analysis_results_Pressure\\ID_4_Schröter_Till_jumps.csv\n",
      "\n",
      "Verarbeite ID_5_Zaschke_Lenard...\n",
      "  REAL_1: 6 Sprünge gefunden\n",
      "  REAL_2: 6 Sprünge gefunden\n",
      "  VR_1: 6 Sprünge gefunden\n",
      "  VR_2: 6 Sprünge gefunden\n",
      "  ✓ Gesamt: 24 Sprünge\n",
      "  ✓ CSV gespeichert: jump_analysis_results_Pressure\\ID_5_Zaschke_Lenard_jumps.csv\n",
      "\n",
      "Verarbeite ID_6_Petroll_Finn...\n",
      "  REAL_1: 6 Sprünge gefunden\n",
      "  REAL_2: 6 Sprünge gefunden\n",
      "  VR_1: 6 Sprünge gefunden\n",
      "  VR_2: 6 Sprünge gefunden\n",
      "  ✓ Gesamt: 24 Sprünge\n",
      "  ✓ CSV gespeichert: jump_analysis_results_Pressure\\ID_6_Petroll_Finn_jumps.csv\n",
      "\n",
      "Verarbeite ID_7_Gruber_Julius...\n",
      "  REAL_1: 6 Sprünge gefunden\n",
      "  REAL_2: 6 Sprünge gefunden\n",
      "  VR_1: 6 Sprünge gefunden\n",
      "  VR_2: 6 Sprünge gefunden\n",
      "  ✓ Gesamt: 24 Sprünge\n",
      "  ✓ CSV gespeichert: jump_analysis_results_Pressure\\ID_7_Gruber_Julius_jumps.csv\n",
      "\n",
      "================================================================================\n",
      "Verarbeitung abgeschlossen!\n",
      "Alle CSVs wurden im Ordner 'jump_analysis_results_Pressure' gespeichert.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "### parameter aus Kraftdaten extrahieren\n",
    "def identify_jumps(df, flight_threshold=50, min_flight_sec=0.2):\n",
    "    \"\"\"\n",
    "    Identifiziert Sprünge über einen Kraft-Schwellenwert und berechnet die Sprunghöhe.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    flight_threshold : float\n",
    "        Kraftwert (N), unter dem eine Flugphase erkannt wird (Default: 50N).\n",
    "    min_flight_sec : float\n",
    "        Mindestdauer in der Luft, um als Sprung zu gelten.\n",
    "    \"\"\"\n",
    "    total_force = (df['LT Force (N)'] + df['RT Force (N)']).values\n",
    "    time = df['time'].values\n",
    "    g = 9.81 # Erdbeschleunigung\n",
    "    \n",
    "    # 1. Flugphasen-Maske\n",
    "    in_air = total_force < flight_threshold\n",
    "    \n",
    "    # 2. Detektion von Statuswechseln\n",
    "    diff = np.diff(in_air.astype(int))\n",
    "    takeoff_indices = np.where(diff == 1)[0]\n",
    "    landing_indices = np.where(diff == -1)[0]\n",
    "    \n",
    "    # Korrektur der Indizes (Start mit Take-off, Ende mit Landung)\n",
    "    if len(takeoff_indices) > 0 and len(landing_indices) > 0:\n",
    "        if landing_indices[0] < takeoff_indices[0]:\n",
    "            landing_indices = landing_indices[1:]\n",
    "        takeoff_indices = takeoff_indices[:len(landing_indices)]\n",
    "\n",
    "    jumps_list = []\n",
    "    \n",
    "    # 3. Flugdaten extrahieren und Höhe berechnen\n",
    "    for i in range(len(takeoff_indices)):\n",
    "        idx_off = takeoff_indices[i]\n",
    "        idx_on = landing_indices[i]\n",
    "        \n",
    "        t_off = time[idx_off]\n",
    "        t_on = time[idx_on]\n",
    "        flug_dauer = t_on - t_off\n",
    "        \n",
    "        # Filter für plausible Sprünge\n",
    "        if flug_dauer > min_flight_sec:\n",
    "            # Berechnung der Sprunghöhe in Metern\n",
    "            # Formel: h = 1/8 * g * t^2\n",
    "            sprunghoehe_m = (1/8) * g * (flug_dauer**2)\n",
    "            sprunghoehe_cm = sprunghoehe_m * 100\n",
    "            \n",
    "            jump_dict = {\n",
    "                'sprung nr.': len(jumps_list) + 1,\n",
    "                't_absprung': round(t_off, 4),\n",
    "                't_landung': round(t_on, 4),\n",
    "                'flugzeit_s': round(flug_dauer, 4),\n",
    "                'sprunghoehe_cm': round(sprunghoehe_cm, 2)\n",
    "            }\n",
    "            jumps_list.append(jump_dict)\n",
    "            \n",
    "    return jumps_list\n",
    "\n",
    "def visualize_jumps(df, jumps_list, participant, measurement):\n",
    "    # Kraft berechnen\n",
    "    total_force = df['LT Force (N)'] + df['RT Force (N)']\n",
    "    time = df['time'].values\n",
    "    \n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.plot(time, total_force, color='black', alpha=0.3, label='Rohdaten Kraft')\n",
    "    \n",
    "    # Hilfsvariablen für die Legende\n",
    "    labeled_flight_zone = False\n",
    "    labeled_events = False\n",
    "    \n",
    "    for j in jumps_list:\n",
    "        # 1. Die Flugphase (t_absprung bis t_landung) markieren\n",
    "        plt.axvspan(j['t_absprung'], j['t_landung'], \n",
    "                    color='orange', alpha=0.2, \n",
    "                    label='Flugphase' if not labeled_flight_zone else \"\")\n",
    "        labeled_flight_zone = True\n",
    "        \n",
    "        # 2. Vertikale Linien für Take-off und Landing\n",
    "        plt.axvline(j['t_absprung'], color='green', linestyle='--', alpha=0.6, \n",
    "                    label='Take-off' if not labeled_events else \"\")\n",
    "        plt.axvline(j['t_landung'], color='red', linestyle='--', alpha=0.6, \n",
    "                    label='Landing' if not labeled_events else \"\")\n",
    "        labeled_events = True\n",
    "        \n",
    "        # 3. Text-Position (Mitte der Flugphase)\n",
    "        text_pos_x = j['t_absprung'] + (j['flugzeit_s'] / 2)\n",
    "        \n",
    "        # 4. Sprungnummer und Sprunghöhe anzeigen\n",
    "        # Wir platzieren den Text etwas über der Kraftkurve\n",
    "        y_pos = max(total_force) * 0.9\n",
    "        plt.text(text_pos_x, y_pos, f\"Sprung {j['sprung nr.']}\\n{j['sprunghoehe_cm']} cm\", \n",
    "                 horizontalalignment='center', fontweight='bold', fontsize=10,\n",
    "                 bbox=dict(facecolor='white', alpha=0.8, edgecolor='orange', boxstyle='round,pad=0.5'))\n",
    "        \n",
    "        # 5. Flugzeit darunter anzeigen\n",
    "        plt.text(text_pos_x, y_pos * 0.82, f\"t: {j['flugzeit_s']}s\", \n",
    "                 horizontalalignment='center', fontsize=9, color='black', alpha=0.7)\n",
    "\n",
    "    # Titel und Labels\n",
    "    title_str = f\"Sprungdetektion: {participant} - {measurement} | {len(jumps_list)} Sprünge gefunden\"\n",
    "    plt.title(title_str, fontsize=14, pad=15)\n",
    "    plt.xlabel(\"Zeit (s)\")\n",
    "    plt.ylabel(\"Gesamtkraft (N)\")\n",
    "    \n",
    "    # Verzeichnis erstellen (falls nicht vorhanden)\n",
    "    save_dir = 'Pictures_Test/Sprungzyklen_Kraft'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True, alpha=0.2)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Speichern\n",
    "    #file_name = f'{participant}_{measurement}_jump_analysis.png'\n",
    "    #plt.savefig(os.path.join(save_dir, file_name), dpi=300)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "output_folder = 'jump_analysis_results_Pressure'\n",
    "\n",
    "# Erstelle Ausgabeverzeichnis, falls nicht vorhanden\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for participant in participants:\n",
    "    print(f\"\\nVerarbeite {participant}...\")\n",
    "    \n",
    "    all_jumps_data = []\n",
    "    \n",
    "    # Durchlaufe alle 4 Messungen\n",
    "    for measurement in measurements:\n",
    "        df = all_data[participant][measurement]\n",
    "        \n",
    "        # Identifiziere Sprünge in dieser Messung\n",
    "        jumps = identify_jumps(df, flight_threshold=155, min_flight_sec=0.2)\n",
    "        #visualize_jumps(df, jumps, participant, measurement)\n",
    "\n",
    "        # Füge Measurement-Info zu jedem Sprung hinzu\n",
    "        for jump in jumps:\n",
    "            jump['messung'] = measurement\n",
    "            all_jumps_data.append(jump)\n",
    "        \n",
    "        print(f\"  {measurement}: {len(jumps)} Sprünge gefunden\")\n",
    "    \n",
    "    # Erstelle DataFrame aus allen Sprüngen\n",
    "    jumps_df = pd.DataFrame(all_jumps_data)\n",
    "    \n",
    "    # Speichere als CSV\n",
    "    output_file = os.path.join(output_folder, f'{participant}_jumps.csv')\n",
    "    jumps_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"  ✓ Gesamt: {len(all_jumps_data)} Sprünge\")\n",
    "    print(f\"  ✓ CSV gespeichert: {output_file}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Verarbeitung abgeschlossen!\")\n",
    "print(f\"Alle CSVs wurden im Ordner '{output_folder}' gespeichert.\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff52ea2",
   "metadata": {},
   "source": [
    "## sprünge nach der Höhe sortieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db04a1a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8de04332",
   "metadata": {},
   "source": [
    "## Zeitnormieren auf sekunde vor takeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "15ff49f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starte Zeit-Normierung (Knie-Logik)...\n",
      "Normiere Daten für ID_1_Dabisch_Samuel...\n",
      "  ✓ REAL_1: 6 Sprünge normiert\n",
      "  ✓ REAL_2: 6 Sprünge normiert\n",
      "  ✓ VR_1: 6 Sprünge normiert\n",
      "  ✓ VR_2: 6 Sprünge normiert\n",
      "Normiere Daten für ID_2_Pohl_Jannis...\n",
      "  ✓ REAL_1: 6 Sprünge normiert\n",
      "  ✓ REAL_2: 5 Sprünge normiert\n",
      "  ✓ VR_1: 6 Sprünge normiert\n",
      "  ✓ VR_2: 1 Sprünge normiert\n",
      "Normiere Daten für ID_3_Kleber_Christian...\n",
      "  ✓ REAL_1: 6 Sprünge normiert\n",
      "  ✓ REAL_2: 6 Sprünge normiert\n",
      "  ✓ VR_1: 6 Sprünge normiert\n",
      "  ✓ VR_2: 6 Sprünge normiert\n",
      "Normiere Daten für ID_4_Schröter_Till...\n",
      "  ✓ REAL_1: 6 Sprünge normiert\n",
      "  ✓ REAL_2: 6 Sprünge normiert\n",
      "  ✓ VR_1: 5 Sprünge normiert\n",
      "  ✓ VR_2: 6 Sprünge normiert\n",
      "Normiere Daten für ID_5_Zaschke_Lenard...\n",
      "  ✓ REAL_1: 6 Sprünge normiert\n",
      "  ✓ REAL_2: 6 Sprünge normiert\n",
      "  ✓ VR_1: 6 Sprünge normiert\n",
      "  ✓ VR_2: 6 Sprünge normiert\n",
      "Normiere Daten für ID_6_Petroll_Finn...\n",
      "  ✓ REAL_1: 6 Sprünge normiert\n",
      "  ✓ REAL_2: 6 Sprünge normiert\n",
      "  ✓ VR_1: 6 Sprünge normiert\n",
      "  ✓ VR_2: 6 Sprünge normiert\n",
      "Normiere Daten für ID_7_Gruber_Julius...\n",
      "  ✓ REAL_1: 6 Sprünge normiert\n",
      "  ✓ REAL_2: 6 Sprünge normiert\n",
      "  ✓ VR_1: 6 Sprünge normiert\n",
      "  ✓ VR_2: 6 Sprünge normiert\n",
      "\n",
      "Fertig! Alle Sprünge sind nun zeitnormiert (Buffer -> Take-off) verfügbar.\n"
     ]
    }
   ],
   "source": [
    "### Zeitnormerierung der Sprünge basierend auf Knie-Extraktion\n",
    "def time_normalize_jumps_knee(df, participant_name, measurement_type, \n",
    "                             jumps_csv_folder='jump_analysis_results_knee', \n",
    "                             normalize_points=100):\n",
    "    \"\"\"\n",
    "    Normiert die Sprünge basierend auf der Knie-Extraktion (start_analyse bis t_absprung).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Der Rohdaten-DataFrame (z.B. df_real1).\n",
    "    participant_name : str\n",
    "        Name des Probanden (für den CSV-Dateinamen).\n",
    "    measurement_type : str\n",
    "        Die aktuelle Messung (z.B. 'REAL_1').\n",
    "    jumps_csv_folder : str\n",
    "        Ordner, in dem die Ergebnisse der Knie-Analyse liegen.\n",
    "    normalize_points : int\n",
    "        Anzahl der Datenpunkte nach der Normierung (Standard: 100).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pfad zur CSV (jetzt aus dem knee-Ordner)\n",
    "    jumps_csv_path = os.path.join(jumps_csv_folder, f'{participant_name}_jumps.csv')\n",
    "    \n",
    "    if not os.path.exists(jumps_csv_path):\n",
    "        print(f\"Warnung: CSV nicht gefunden {jumps_csv_path}\")\n",
    "        return {}\n",
    "    \n",
    "    # 1. CSV laden\n",
    "    jumps_info = pd.read_csv(jumps_csv_path)\n",
    "    \n",
    "    # 2. Filter auf die aktuelle Messung (z.B. nur REAL_1)\n",
    "    relevant_jumps = jumps_info[jumps_info['messung'] == measurement_type]\n",
    "    \n",
    "    normalized_jumps = {}\n",
    "    \n",
    "    # 3. Über die gefundenen Sprünge iterieren\n",
    "    for idx, row in relevant_jumps.iterrows():\n",
    "        # Zeitgrenzen aus deiner neuen Struktur\n",
    "        start_time = row['start_analyse']\n",
    "        end_time = row['t_absprung']\n",
    "        jump_nr = int(row['sprung nr.'])\n",
    "        \n",
    "        # Daten im Zeitfenster ausschneiden\n",
    "        mask = (df['time'] >= start_time) & (df['time'] <= end_time)\n",
    "        jump_data = df[mask].reset_index(drop=True)\n",
    "        \n",
    "        # Sicherheitscheck falls keine Daten im Fenster sind\n",
    "        if len(jump_data) < 2:\n",
    "            continue\n",
    "            \n",
    "        # --- Zeit-Normierung (Interpolation) ---\n",
    "        # Erstelle Indizes für die ursprünglichen Daten (z.B. 0 bis 145)\n",
    "        original_indices = np.linspace(0, len(jump_data) - 1, len(jump_data))\n",
    "        # Erstelle Indizes für die Ziel-Punkte (z.B. 0 bis 99)\n",
    "        new_indices = np.linspace(0, len(jump_data) - 1, normalize_points)\n",
    "        \n",
    "        normalized_data = {}\n",
    "        for col in jump_data.columns:\n",
    "            # Nur numerische Spalten (Kraft, Winkel, EMG) interpolieren\n",
    "            if pd.api.types.is_numeric_dtype(jump_data[col]):\n",
    "                normalized_data[col] = np.interp(new_indices, original_indices, jump_data[col].values)\n",
    "            else:\n",
    "                # Nicht-numerische Spalten (z.B. ID) einfach den ersten Wert beibehalten\n",
    "                normalized_data[col] = [jump_data[col].iloc[0]] * normalize_points\n",
    "        \n",
    "        # Füge die normierte Zeitachse (0% bis 100%) hinzu\n",
    "        normalized_data['time_normalized'] = np.linspace(0, 1, normalize_points)\n",
    "        \n",
    "        # In DataFrame umwandeln\n",
    "        normalized_df = pd.DataFrame(normalized_data)\n",
    "        \n",
    "        # Eindeutiger Key für das Dictionary\n",
    "        key = f\"{measurement_type}_jump_{jump_nr}\"\n",
    "        normalized_jumps[key] = normalized_df\n",
    "    \n",
    "    return normalized_jumps\n",
    "\n",
    "# %% Zeit-Normierung basierend auf Knie-Extraktion\n",
    "print(f\"\\nStarte Zeit-Normierung (Knie-Logik)...\")\n",
    "\n",
    "# Neues Zielverzeichnis für die Knie-Ergebnisse\n",
    "output_folder_knee = 'jump_analysis_results_knee'\n",
    "\n",
    "all_normalized_data = {}\n",
    "\n",
    "for participant in participants:\n",
    "    print(f\"Normiere Daten für {participant}...\")\n",
    "    all_normalized_data[participant] = {}\n",
    "    \n",
    "    for measurement in measurements:\n",
    "        df = all_data[participant][measurement]\n",
    "        \n",
    "        # Wichtig: Nutze hier die neue Funktion, die 'start_analyse' und 't_absprung' liest\n",
    "        norm_jumps_dict = time_normalize_jumps_knee(\n",
    "            df, \n",
    "            participant, \n",
    "            measurement, \n",
    "            jumps_csv_folder=output_folder_knee, # Geänderter Ordner\n",
    "            normalize_points=100\n",
    "        )\n",
    "        \n",
    "        # Speichern im Haupt-Dictionary\n",
    "        all_normalized_data[participant].update(norm_jumps_dict)\n",
    "        \n",
    "        print(f\"  ✓ {measurement}: {len(norm_jumps_dict)} Sprünge normiert\")\n",
    "\n",
    "print(f\"\\nFertig! Alle Sprünge sind nun zeitnormiert (Buffer -> Take-off) verfügbar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363c6c95",
   "metadata": {},
   "source": [
    "## Knie, Hüft und Sprunggelenk kurven pro teilneher und über alle teilnehmer + mittelwert/ standartabweichung "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7f929e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plotten und speichern der normalisierten Bedingungen\n",
    "def plot_normalized_conditions(normalized_dict, participant_name, condition_type, column_to_plot):\n",
    "    \"\"\"\n",
    "    Fasst alle Sprünge einer Bedingung (z.B. REAL_1 + REAL_2) zusammen.\n",
    "    Erstellt:\n",
    "    1. Ein großes Subplot-Bild mit allen ~12 Sprüngen.\n",
    "    2. Ein Vergleichsbild (Overlaid) mit dem Gesamt-Mittelwert der Bedingung.\n",
    "    \"\"\"\n",
    "    # 1. Vorbereitung Ordnerstruktur\n",
    "    target_dir = os.path.join('Pictures_Test', 'Conditions_Combined', column_to_plot.replace(' ', '_'))\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    \n",
    "    # Filtere die Keys: Sucht nach allen Keys, die den Bedingungs-String enthalten\n",
    "    # z.B. wenn condition_type='REAL', findet er 'REAL_1_jump_1' bis 'REAL_2_jump_6'\n",
    "    jump_keys = [k for k in normalized_dict.keys() if condition_type in k]\n",
    "    num_jumps = len(jump_keys)\n",
    "    \n",
    "    if num_jumps == 0:\n",
    "        print(f\"Keine Daten für Bedingung {condition_type} bei {participant_name} gefunden.\")\n",
    "        return\n",
    "\n",
    "    # Daten sammeln\n",
    "    all_values = []\n",
    "    time_normalized = None\n",
    "\n",
    "    # --- GRAFIK 1: SUBPLOTS (Alle 12 untereinander) ---\n",
    "    # Wir machen das Layout etwas kompakter, da 12 Sprünge viel Platz brauchen\n",
    "    fig_sub, axes = plt.subplots(num_jumps, 1, figsize=(10, 2 * num_jumps), sharex=True)\n",
    "    if num_jumps == 1: axes = [axes]\n",
    "    \n",
    "    for i, key in enumerate(jump_keys):\n",
    "        df_jump = normalized_dict[key]\n",
    "        val = df_jump[column_to_plot].values\n",
    "        all_values.append(val)\n",
    "        if time_normalized is None:\n",
    "            time_normalized = df_jump['time_normalized'].values * 100 # In Prozent umrechnen\n",
    "            \n",
    "        axes[i].plot(time_normalized, val, color='black', lw=1)\n",
    "        axes[i].set_ylabel(\"Wert\", fontsize=8)\n",
    "        axes[i].set_title(f\"{key}\", fontsize=9, pad=2)\n",
    "        axes[i].grid(True, alpha=0.2)\n",
    "    \n",
    "    plt.xlabel(\"Zeitverlauf Normiert (0-100% vor Takeoff)\")\n",
    "    plt.tight_layout()\n",
    "    fig_sub.savefig(os.path.join(target_dir, f\"{participant_name}_{condition_type}_all_subplots.png\"))\n",
    "    plt.close(fig_sub)\n",
    "\n",
    "    # --- GRAFIK 2: ÜBEREINANDERGELEGT (12 Sprünge + 1 dicker Mittelwert) ---\n",
    "    plt.figure(figsize=(11, 7))\n",
    "    \n",
    "    data_matrix = np.array(all_values)\n",
    "    mean_val = np.mean(data_matrix, axis=0)\n",
    "    std_val = np.std(data_matrix, axis=0)\n",
    "\n",
    "    # Einzelne Sprünge (sehr blass im Hintergrund)\n",
    "    for i, val in enumerate(all_values):\n",
    "        plt.plot(time_normalized, val, alpha=0.15, lw=1, color='gray')\n",
    "    \n",
    "    # Mittelwert (Fett in Farbe je nach Bedingung)\n",
    "    color = 'blue' if 'REAL' in condition_type else 'red'\n",
    "    plt.plot(time_normalized, mean_val, color=color, lw=4, label=f'Gesamt-Mittelwert {condition_type} (n={num_jumps})')\n",
    "    \n",
    "    # Standardabweichung (Schatten)\n",
    "    plt.fill_between(time_normalized, mean_val - std_val, mean_val + std_val, \n",
    "                     color=color, alpha=0.2, label=f'Standardabweichung (±1 SD)')\n",
    "    \n",
    "    plt.title(f\"Bedingungs-Analyse: {participant_name}\\nVariable: {column_to_plot} | Bedingung: {condition_type}\", fontsize=12)\n",
    "    plt.xlabel(\"Vorbereitungsphase in % (0% = Start, 100% = Takeoff)\")\n",
    "    plt.ylabel(column_to_plot)\n",
    "    plt.legend(loc='upper left', frameon=True)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Speichern Overlaid\n",
    "    plt.savefig(os.path.join(target_dir, f\"{participant_name}_{condition_type}_combined_mean.png\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    #print(f\"  ✓ Kombinierte {condition_type}-Grafiken (n={num_jumps}) gespeichert in: {target_dir}\")\n",
    "\n",
    "### Plot funkton für einen Winkel\n",
    "def plot_everything_for_one_column_to_plot (column_to_plot):\n",
    "    \"\"\"\n",
    "    Wrapper-Funktion, um für alle Teilnehmer und eine bestimmte Spalte die \n",
    "    kombinierten Plots zu erstellen.\n",
    "    \"\"\"\n",
    "    print(f\"\\nStarte Generierung der kombinierten Plots (12 Sprünge pro Bedingung)...\")\n",
    "\n",
    "    for participant in participants:\n",
    "        #print(f\"\\nErstelle kombinierte Plots für {participant} - Variable: {column_to_plot}\")\n",
    "        \n",
    "        # Bedingungen REAL und VR\n",
    "        for condition in ['REAL', 'VR']:\n",
    "            plot_normalized_conditions(\n",
    "                all_normalized_data[participant], \n",
    "                participant, \n",
    "                condition, \n",
    "                column_to_plot\n",
    "            )\n",
    "\n",
    "        # print(f\"  ✓ Bedingungen REAL und VR für {participant} erfolgreich zusammengefasst.\")\n",
    "    \n",
    "    print(f\"\\n{'='*30}\")\n",
    "    print(\"ALLE KOMBINIERTEN GRAFIKEN ERSTELLT\")\n",
    "    print(f\"Speicherort: Pictures_Test/Conditions_Combined/{column_to_plot.replace(' ', '_').replace('(', '').replace(')', '')}/\")\n",
    "    print(f\"{'='*30}\")  \n",
    "\n",
    "### globaler plot für alle Teilnehmer einer Bedingung\n",
    "def plot_global_condition_comparison(all_participants_dict, condition_type, column_to_plot):\n",
    "    \"\"\"\n",
    "    Erstellt einen Plot für ALLE Teilnehmer zusammengefasst für eine Bedingung (REAL oder VR).\n",
    "    - Dünne graue Linien: Jeder einzelne Sprung (n=~84)\n",
    "    - Dicke farbige Linie: Gesamt-Mittelwert über alle Teilnehmer\n",
    "    - Schattierter Bereich: Standardabweichung\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Vorbereitung Ordnerstruktur\n",
    "    target_dir = os.path.join('Pictures_Test', 'Global_Comparison', column_to_plot.replace(' ', '_'))\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    \n",
    "    all_values = []\n",
    "    time_normalized = None\n",
    "    total_jumps_count = 0\n",
    "\n",
    "    # 2. Daten sammeln über alle Teilnehmer hinweg\n",
    "    for p_name, normalized_dict in all_participants_dict.items():\n",
    "        # Suche Sprünge für die Bedingung (z.B. REAL) bei diesem Teilnehmer\n",
    "        jump_keys = [k for k in normalized_dict.keys() if condition_type in k]\n",
    "        \n",
    "        for key in jump_keys:\n",
    "            df_jump = normalized_dict[key]\n",
    "            val = df_jump[column_to_plot].values\n",
    "            all_values.append(val)\n",
    "            \n",
    "            if time_normalized is None:\n",
    "                # In Prozent umrechnen (0-100)\n",
    "                time_normalized = df_jump['time_normalized'].values * 100\n",
    "            \n",
    "            total_jumps_count += 1\n",
    "\n",
    "    if total_jumps_count == 0:\n",
    "        print(f\"Keine Daten für {condition_type} in der gesamten Gruppe gefunden.\")\n",
    "        return\n",
    "\n",
    "    # 3. Statistik berechnen\n",
    "    data_matrix = np.array(all_values)\n",
    "    mean_val = np.mean(data_matrix, axis=0)\n",
    "    std_val = np.std(data_matrix, axis=0)\n",
    "\n",
    "    # 4. Plotting\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Einzelne Sprünge (84 dünne Linien)\n",
    "    for val in all_values:\n",
    "        plt.plot(time_normalized, val, alpha=0.08, lw=0.8, color='gray') # Sehr blass für die Masse\n",
    "    \n",
    "    # Farbe festlegen\n",
    "    main_color = 'blue' if 'REAL' in condition_type else 'red'\n",
    "    \n",
    "    # Standardabweichung (Schatten)\n",
    "    plt.fill_between(time_normalized, mean_val - std_val, mean_val + std_val, \n",
    "                     color=main_color, alpha=0.15, label=f'SD (±1 σ)')\n",
    "    \n",
    "    # Mittelwert (Fett)\n",
    "    plt.plot(time_normalized, mean_val, color=main_color, lw=3.5, \n",
    "             label=f'Gesamt-Mittelwert {condition_type} (n={total_jumps_count} Sprünge)')\n",
    "    \n",
    "    # Layout & Beschriftung\n",
    "    plt.title(f\"Globale Analyse: Alle Teilnehmer\\nVariable: {column_to_plot} | Bedingung: {condition_type}\", fontsize=14)\n",
    "    plt.xlabel(\"Bewegungszyklus in % (0% = Start, 100% = Takeoff)\", fontsize=12)\n",
    "    plt.ylabel(column_to_plot, fontsize=12)\n",
    "    plt.xlim(0, 100)\n",
    "    plt.legend(loc='best', frameon=True)\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Speichern\n",
    "    file_name = f\"GLOBAL_{condition_type}_{column_to_plot.replace(' ', '_')}.png\"\n",
    "    plt.savefig(os.path.join(target_dir, file_name), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"  ✓ Globaler Plot gespeichert: {file_name} (n={total_jumps_count} Sprünge)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb7fb1d",
   "metadata": {},
   "source": [
    "### Gelenke pro Teilnehmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aaa1da9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starte Generierung der kombinierten Plots (12 Sprünge pro Bedingung)...\n",
      "\n",
      "==============================\n",
      "ALLE KOMBINIERTEN GRAFIKEN ERSTELLT\n",
      "Speicherort: Pictures_Test/Conditions_Combined/RT_Knee_Flexion_deg/\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# plots für Knie aufrufen\n",
    "colum_to_Plot = 'RT Knee Flexion (deg)'\n",
    "plot_everything_for_one_column_to_plot(colum_to_Plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d62e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots für Hüfte aufrufen\n",
    "colum_to_Plot = 'RT Hip Flexion (deg)'\n",
    "plot_everything_for_one_column_to_plot(colum_to_Plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3b9cf180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starte Generierung der kombinierten Plots (12 Sprünge pro Bedingung)...\n",
      "\n",
      "==============================\n",
      "ALLE KOMBINIERTEN GRAFIKEN ERSTELLT\n",
      "Speicherort: Pictures_Test/Conditions_Combined/RT_Ankle_Dorsiflexion_deg/\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# plots für Sprunggelenk aufrufen\n",
    "colum_to_Plot = 'RT Ankle Dorsiflexion (deg)'\n",
    "plot_everything_for_one_column_to_plot(colum_to_Plot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27791d3a",
   "metadata": {},
   "source": [
    "### Globaler Plot mit allen Teilnehmern zusammen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9a16222a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'time_normalized'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'time_normalized'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m### globale plots für Knie vergleichen\u001b[39;00m\n\u001b[0;32m      2\u001b[0m colum_to_Plot \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRT Knee Flexion (deg)\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mplot_global_condition_comparison\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mREAL\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolum_to_Plot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m plot_global_condition_comparison(all_data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVR\u001b[39m\u001b[38;5;124m'\u001b[39m, colum_to_Plot)\n",
      "Cell \u001b[1;32mIn[60], line 135\u001b[0m, in \u001b[0;36mplot_global_condition_comparison\u001b[1;34m(all_participants_dict, condition_type, column_to_plot)\u001b[0m\n\u001b[0;32m    131\u001b[0m         all_values\u001b[38;5;241m.\u001b[39mappend(val)\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m time_normalized \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    134\u001b[0m             \u001b[38;5;66;03m# In Prozent umrechnen (0-100)\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m             time_normalized \u001b[38;5;241m=\u001b[39m \u001b[43mdf_jump\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtime_normalized\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m    137\u001b[0m         total_jumps_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_jumps_count \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'time_normalized'"
     ]
    }
   ],
   "source": [
    "### globale plots für Knie vergleichen\n",
    "colum_to_Plot = 'RT Knee Flexion (deg)'\n",
    "plot_global_condition_comparison(all_data, 'REAL', colum_to_Plot)\n",
    "plot_global_condition_comparison(all_data, 'VR', colum_to_Plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaf586a",
   "metadata": {},
   "source": [
    "## Parameter Bestimmen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187f926e",
   "metadata": {},
   "source": [
    "### Maximaler Winkel innerhalb der Fenster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24ce7a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starte Extraktion der Maximalwerte...\n",
      "Verarbeite Parameter für ID_1_Dabisch_Samuel:\n",
      "  ✓ Spalte 'RT_Knee_Max' erfolgreich aktualisiert.\n",
      "  ✓ Spalte 'LT_Knee_Max' erfolgreich aktualisiert.\n",
      "  ✓ Spalte 'RT_Hip_Max' erfolgreich aktualisiert.\n",
      "  ✓ Spalte 'LT_Hip_Max' erfolgreich aktualisiert.\n",
      "  ✓ Spalte 'RT_Ankle_Max' erfolgreich aktualisiert.\n",
      "  ✓ Spalte 'LT_Ankle_Max' erfolgreich aktualisiert.\n",
      "Verarbeite Parameter für ID_2_Pohl_Jannis:\n",
      "  ✓ Spalte 'RT_Knee_Max' erfolgreich aktualisiert.\n",
      "  ✓ Spalte 'LT_Knee_Max' erfolgreich aktualisiert.\n",
      "  ✓ Spalte 'RT_Hip_Max' erfolgreich aktualisiert.\n",
      "  ✓ Spalte 'LT_Hip_Max' erfolgreich aktualisiert.\n",
      "  ✓ Spalte 'RT_Ankle_Max' erfolgreich aktualisiert.\n",
      "  ✓ Spalte 'LT_Ankle_Max' erfolgreich aktualisiert.\n",
      "Verarbeite Parameter für ID_3_Kleber_Christian:\n",
      "  ✓ Spalte 'RT_Knee_Max' erfolgreich aktualisiert.\n",
      "  ✓ Spalte 'LT_Knee_Max' erfolgreich aktualisiert.\n",
      "  ✓ Spalte 'RT_Hip_Max' erfolgreich aktualisiert.\n",
      "  ✓ Spalte 'LT_Hip_Max' erfolgreich aktualisiert.\n",
      "  ✓ Spalte 'RT_Ankle_Max' erfolgreich aktualisiert.\n",
      "  ✓ Spalte 'LT_Ankle_Max' erfolgreich aktualisiert.\n",
      "Verarbeite Parameter für ID_4_Schröter_Till:\n",
      "  ✓ Spalte 'RT_Knee_Max' erfolgreich aktualisiert.\n",
      "  ✓ Spalte 'LT_Knee_Max' erfolgreich aktualisiert.\n",
      "  ✓ Spalte 'RT_Hip_Max' erfolgreich aktualisiert.\n",
      "  ✓ Spalte 'LT_Hip_Max' erfolgreich aktualisiert.\n",
      "  ✓ Spalte 'RT_Ankle_Max' erfolgreich aktualisiert.\n",
      "  ✓ Spalte 'LT_Ankle_Max' erfolgreich aktualisiert.\n",
      "Verarbeite Parameter für ID_5_Zaschke_Lenard:\n",
      "  ✓ Spalte 'RT_Knee_Max' erfolgreich aktualisiert.\n",
      "  ✓ Spalte 'LT_Knee_Max' erfolgreich aktualisiert.\n",
      "  ✓ Spalte 'RT_Hip_Max' erfolgreich aktualisiert.\n",
      "  ✓ Spalte 'LT_Hip_Max' erfolgreich aktualisiert.\n",
      "  ✓ Spalte 'RT_Ankle_Max' erfolgreich aktualisiert.\n",
      "  ✓ Spalte 'LT_Ankle_Max' erfolgreich aktualisiert.\n",
      "Verarbeite Parameter für ID_6_Petroll_Finn:\n",
      "  ✓ Spalte 'RT_Knee_Max' erfolgreich aktualisiert.\n",
      "  ✓ Spalte 'LT_Knee_Max' erfolgreich aktualisiert.\n",
      "  ✓ Spalte 'RT_Hip_Max' erfolgreich aktualisiert.\n",
      "  ✓ Spalte 'LT_Hip_Max' erfolgreich aktualisiert.\n",
      "  ✓ Spalte 'RT_Ankle_Max' erfolgreich aktualisiert.\n",
      "  ✓ Spalte 'LT_Ankle_Max' erfolgreich aktualisiert.\n",
      "Verarbeite Parameter für ID_7_Gruber_Julius:\n",
      "  ✓ Spalte 'RT_Knee_Max' erfolgreich aktualisiert.\n",
      "  ✓ Spalte 'LT_Knee_Max' erfolgreich aktualisiert.\n",
      "  ✓ Spalte 'RT_Hip_Max' erfolgreich aktualisiert.\n",
      "  ✓ Spalte 'LT_Hip_Max' erfolgreich aktualisiert.\n",
      "  ✓ Spalte 'RT_Ankle_Max' erfolgreich aktualisiert.\n",
      "  ✓ Spalte 'LT_Ankle_Max' erfolgreich aktualisiert.\n",
      "\n",
      "==============================\n",
      "ALLE PARAMETER GESPEICHERT IM ORDNER 'Parameter_IMU'\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "### Parameter extrahieren\n",
    "def extract_max_parameter(normalized_dict, column_to_process):\n",
    "    \"\"\"\n",
    "    Extrahiert den Maximalwert einer Spalte für jeden Sprung.\n",
    "    Gibt ein Dictionary zurück: {'REAL_1_jump_1': 65.4, ...}\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for jump_key, df_jump in normalized_dict.items():\n",
    "        if column_to_process in df_jump.columns:\n",
    "            max_val = df_jump[column_to_process].max()\n",
    "            results[jump_key] = round(max_val, 2)\n",
    "    \n",
    "    type = 'Maximalwert'\n",
    "    return results, type\n",
    "\n",
    "### Parameter speichern/aktualisieren\n",
    "def update_participant_parameters(participant_name, new_data_dict, column_label, type):\n",
    "    \"\"\"\n",
    "    Speichert Parameter in einer CSV. \n",
    "    Garantiert korrekte Spaltenbenennung und verhindert KeyErrors beim Mergen.\n",
    "    \"\"\"\n",
    "    folder = 'Parameter_IMU'\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    file_path = os.path.join(folder, f'{participant_name}_parameter_{type}.csv')\n",
    "    \n",
    "    # 1. Neuen DataFrame aus dem Dictionary erstellen\n",
    "    rows = []\n",
    "    for jump_key, value in new_data_dict.items():\n",
    "        if '_jump_' in jump_key:\n",
    "            parts = jump_key.split('_jump_')\n",
    "            trial = parts[0]\n",
    "            sprung_nr = int(parts[1]) # Als Zahl für saubere Sortierung\n",
    "        else:\n",
    "            trial, sprung_nr = jump_key, 0\n",
    "        \n",
    "        rows.append({\n",
    "            'participant': participant_name,\n",
    "            'trial': trial,\n",
    "            'sprung_nr': sprung_nr,\n",
    "            column_label: value\n",
    "        })\n",
    "    \n",
    "    new_df = pd.DataFrame(rows)\n",
    "    \n",
    "    # 2. Falls Datei existiert, einlesen und mergen\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            existing_df = pd.read_csv(file_path, sep=';')\n",
    "            \n",
    "            # Sicherheitscheck: Falls 'participant' fehlt, wurde die CSV falsch erstellt\n",
    "            if 'participant' not in existing_df.columns:\n",
    "                print(f\"  ! CSV von {participant_name} war fehlerhaft. Wird neu erstellt.\")\n",
    "                new_df.to_csv(file_path, sep=';', index=False)\n",
    "                return\n",
    "\n",
    "            # Falls die Spalte schon existiert, im alten DF löschen (für das Update)\n",
    "            if column_label in existing_df.columns:\n",
    "                existing_df = existing_df.drop(columns=[column_label])\n",
    "            \n",
    "            # Mergen (Zusammenführen)\n",
    "            updated_df = pd.merge(\n",
    "                existing_df, \n",
    "                new_df, \n",
    "                on=['participant', 'trial', 'sprung_nr'], \n",
    "                how='outer'\n",
    "            )\n",
    "            \n",
    "            # Sortieren (Real_1 vor Real_2, Sprung 1 vor Sprung 2)\n",
    "            updated_df = updated_df.sort_values(by=['trial', 'sprung_nr'])\n",
    "            updated_df.to_csv(file_path, sep=';', index=False)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ! Fehler beim Update von {participant_name}: {e}. Erstelle Datei neu.\")\n",
    "            new_df.to_csv(file_path, sep=';', index=False)\n",
    "    else:\n",
    "        # 3. Datei existiert noch nicht: Neu erstellen\n",
    "        new_df.to_csv(file_path, sep=';', index=False)\n",
    "\n",
    "    print(f\"  ✓ Spalte '{column_label}' erfolgreich aktualisiert.\")\n",
    "\n",
    "\n",
    "# %% Parameter-Extraktion (Maximalwerte) --------------------------------------\n",
    "print(f\"\\nStarte Extraktion der Maximalwerte...\")\n",
    "\n",
    "# Hier definierst du, welche Spalten du auswerten willst\n",
    "# Der Key ist die Spalte im DF, der Value ist der Name für deine CSV\n",
    "columns_to_analyze = {\n",
    "    'RT Knee Flexion (deg)': 'RT_Knee_Max',\n",
    "    'LT Knee Flexion (deg)': 'LT_Knee_Max',\n",
    "    'RT Hip Flexion (deg)': 'RT_Hip_Max',\n",
    "    'LT Hip Flexion (deg)': 'LT_Hip_Max',\n",
    "    'RT Ankle Dorsiflexion (deg)': 'RT_Ankle_Max',\n",
    "    'LT Ankle Dorsiflexion (deg)': 'LT_Ankle_Max'\n",
    "}\n",
    "\n",
    "for participant in participants:\n",
    "    print(f\"Verarbeite Parameter für {participant}:\")\n",
    "    \n",
    "    for raw_col, csv_label in columns_to_analyze.items():\n",
    "        # 1. Maxima berechnen\n",
    "        max_values, type  = extract_max_parameter(all_normalized_data[participant], raw_col)\n",
    "        \n",
    "        # 2. CSV updaten/erstellen\n",
    "        if max_values: # Nur wenn Daten gefunden wurden\n",
    "            update_participant_parameters(participant, max_values, csv_label, type)\n",
    "\n",
    "print(f\"\\n{'='*30}\")\n",
    "print(f\"ALLE PARAMETER GESPEICHERT IM ORDNER 'Parameter_IMU'\")\n",
    "print(f\"{'='*30}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
