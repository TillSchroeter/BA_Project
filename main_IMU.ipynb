{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "521b9f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%-------------------------------------------------------------------------------------------\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt, iirnotch\n",
    "import glob\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "\n",
    "import functions_IMU\n",
    "import functions_generell\n",
    "import functions_PRESSURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c8897da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daten f√ºr ID_1_Dabisch_Samuel geladen\n",
      "Daten f√ºr ID_2_Pohl_Jannis geladen\n",
      "Daten f√ºr ID_3_Kleber_Christian geladen\n",
      "Daten f√ºr ID_4_Schr√∂ter_Till geladen\n",
      "Daten f√ºr ID_5_Zaschke_Lenard geladen\n",
      "Daten f√ºr ID_6_Petroll_Finn geladen\n",
      "Daten f√ºr ID_7_Gruber_Julius geladen\n",
      "\n",
      "Alle Daten geladen! 7 Teilnehmer erfolgreich eingelesen.\n"
     ]
    }
   ],
   "source": [
    "#%% Daten einlesen f√ºr alle Teilnehmer-------------------------------------------------------------------------------------------\n",
    "# liste der Teilneher\n",
    "participants = ['ID_1_Dabisch_Samuel', 'ID_2_Pohl_Jannis', 'ID_3_Kleber_Christian',\n",
    "                'ID_4_Schr√∂ter_Till', 'ID_5_Zaschke_Lenard', 'ID_6_Petroll_Finn', 'ID_7_Gruber_Julius']\n",
    "# bedingunegn der Messungen\n",
    "measurements = ['REAL_1', 'REAL_2', 'VR_1', 'VR_2']\n",
    "\n",
    "# Alle Daten der Teilnehmer einlesen und verarbeiten\n",
    "all_data = {}\n",
    "\n",
    "for participant in participants:\n",
    "    folder_path = os.path.join('data_final', participant)\n",
    "    \n",
    "    # Lade die 6 CSV-Dateien f√ºr den Teilnehmer\n",
    "    real_1, real_2, vr_1, vr_2, mvc_beine, mvc_hals = functions_generell.load_csvs(folder_path)\n",
    "    \n",
    "    # Speichere die DataFrames im Dictionary\n",
    "    all_data[participant] = {\n",
    "        'REAL_1': real_1,\n",
    "        'REAL_2': real_2,\n",
    "        'VR_1': vr_1,\n",
    "        'VR_2': vr_2,\n",
    "        'MVC_Beine': mvc_beine,\n",
    "        'MVC_Hals': mvc_hals\n",
    "    }\n",
    "    \n",
    "    print(f\"Daten f√ºr {participant} geladen\")\n",
    "\n",
    "print(f\"\\nAlle Daten geladen! {len(all_data)} Teilnehmer erfolgreich eingelesen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3cb7fb",
   "metadata": {},
   "source": [
    "## Hier werden die Jump parameter (dauer, H√∂he) anhand der Kraft daten bestimmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90214282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verarbeite ID_1_Dabisch_Samuel...\n",
      "  REAL_1: 6 Spr√ºnge gefunden\n",
      "  REAL_2: 6 Spr√ºnge gefunden\n",
      "  VR_1: 6 Spr√ºnge gefunden\n",
      "  VR_2: 6 Spr√ºnge gefunden\n",
      "  ‚úì Gesamt: 24 Spr√ºnge\n",
      "  ‚úì CSV gespeichert: jump_analysis_results_Pressure\\ID_1_Dabisch_Samuel_jumps.csv\n",
      "\n",
      "Verarbeite ID_2_Pohl_Jannis...\n",
      "  REAL_1: 6 Spr√ºnge gefunden\n",
      "  REAL_2: 5 Spr√ºnge gefunden\n",
      "  VR_1: 6 Spr√ºnge gefunden\n",
      "  VR_2: 2 Spr√ºnge gefunden\n",
      "  ‚úì Gesamt: 19 Spr√ºnge\n",
      "  ‚úì CSV gespeichert: jump_analysis_results_Pressure\\ID_2_Pohl_Jannis_jumps.csv\n",
      "\n",
      "Verarbeite ID_3_Kleber_Christian...\n",
      "  REAL_1: 6 Spr√ºnge gefunden\n",
      "  REAL_2: 6 Spr√ºnge gefunden\n",
      "  VR_1: 6 Spr√ºnge gefunden\n",
      "  VR_2: 6 Spr√ºnge gefunden\n",
      "  ‚úì Gesamt: 24 Spr√ºnge\n",
      "  ‚úì CSV gespeichert: jump_analysis_results_Pressure\\ID_3_Kleber_Christian_jumps.csv\n",
      "\n",
      "Verarbeite ID_4_Schr√∂ter_Till...\n",
      "  REAL_1: 6 Spr√ºnge gefunden\n",
      "  REAL_2: 6 Spr√ºnge gefunden\n",
      "  VR_1: 6 Spr√ºnge gefunden\n",
      "  VR_2: 6 Spr√ºnge gefunden\n",
      "  ‚úì Gesamt: 24 Spr√ºnge\n",
      "  ‚úì CSV gespeichert: jump_analysis_results_Pressure\\ID_4_Schr√∂ter_Till_jumps.csv\n",
      "\n",
      "Verarbeite ID_5_Zaschke_Lenard...\n",
      "  REAL_1: 6 Spr√ºnge gefunden\n",
      "  REAL_2: 6 Spr√ºnge gefunden\n",
      "  VR_1: 6 Spr√ºnge gefunden\n",
      "  VR_2: 6 Spr√ºnge gefunden\n",
      "  ‚úì Gesamt: 24 Spr√ºnge\n",
      "  ‚úì CSV gespeichert: jump_analysis_results_Pressure\\ID_5_Zaschke_Lenard_jumps.csv\n",
      "\n",
      "Verarbeite ID_6_Petroll_Finn...\n",
      "  REAL_1: 6 Spr√ºnge gefunden\n",
      "  REAL_2: 6 Spr√ºnge gefunden\n",
      "  VR_1: 6 Spr√ºnge gefunden\n",
      "  VR_2: 6 Spr√ºnge gefunden\n",
      "  ‚úì Gesamt: 24 Spr√ºnge\n",
      "  ‚úì CSV gespeichert: jump_analysis_results_Pressure\\ID_6_Petroll_Finn_jumps.csv\n",
      "\n",
      "Verarbeite ID_7_Gruber_Julius...\n",
      "  REAL_1: 6 Spr√ºnge gefunden\n",
      "  REAL_2: 6 Spr√ºnge gefunden\n",
      "  VR_1: 6 Spr√ºnge gefunden\n",
      "  VR_2: 6 Spr√ºnge gefunden\n",
      "  ‚úì Gesamt: 24 Spr√ºnge\n",
      "  ‚úì CSV gespeichert: jump_analysis_results_Pressure\\ID_7_Gruber_Julius_jumps.csv\n",
      "\n",
      "================================================================================\n",
      "Verarbeitung abgeschlossen!\n",
      "Alle CSVs wurden im Ordner 'jump_analysis_results_Pressure' gespeichert.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "### parameter aus Kraftdaten extrahieren\n",
    "def identify_jumps(df, flight_threshold=50, min_flight_sec=0.2):\n",
    "    \"\"\"\n",
    "    Identifiziert Spr√ºnge √ºber einen Kraft-Schwellenwert und berechnet die Sprungh√∂he.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    flight_threshold : float\n",
    "        Kraftwert (N), unter dem eine Flugphase erkannt wird (Default: 50N).\n",
    "    min_flight_sec : float\n",
    "        Mindestdauer in der Luft, um als Sprung zu gelten.\n",
    "    \"\"\"\n",
    "    total_force = (df['LT Force (N)'] + df['RT Force (N)']).values\n",
    "    time = df['time'].values\n",
    "    g = 9.81 # Erdbeschleunigung\n",
    "    \n",
    "    # 1. Flugphasen-Maske\n",
    "    in_air = total_force < flight_threshold\n",
    "    \n",
    "    # 2. Detektion von Statuswechseln\n",
    "    diff = np.diff(in_air.astype(int))\n",
    "    takeoff_indices = np.where(diff == 1)[0]\n",
    "    landing_indices = np.where(diff == -1)[0]\n",
    "    \n",
    "    # Korrektur der Indizes (Start mit Take-off, Ende mit Landung)\n",
    "    if len(takeoff_indices) > 0 and len(landing_indices) > 0:\n",
    "        if landing_indices[0] < takeoff_indices[0]:\n",
    "            landing_indices = landing_indices[1:]\n",
    "        takeoff_indices = takeoff_indices[:len(landing_indices)]\n",
    "\n",
    "    jumps_list = []\n",
    "    \n",
    "    # 3. Flugdaten extrahieren und H√∂he berechnen\n",
    "    for i in range(len(takeoff_indices)):\n",
    "        idx_off = takeoff_indices[i]\n",
    "        idx_on = landing_indices[i]\n",
    "        \n",
    "        t_off = time[idx_off]\n",
    "        t_on = time[idx_on]\n",
    "        flug_dauer = t_on - t_off\n",
    "        \n",
    "        # Filter f√ºr plausible Spr√ºnge\n",
    "        if flug_dauer > min_flight_sec:\n",
    "            # Berechnung der Sprungh√∂he in Metern\n",
    "            # Formel: h = 1/8 * g * t^2\n",
    "            sprunghoehe_m = (1/8) * g * (flug_dauer**2)\n",
    "            sprunghoehe_cm = sprunghoehe_m * 100\n",
    "            \n",
    "            jump_dict = {\n",
    "                'sprung nr.': len(jumps_list) + 1,\n",
    "                't_absprung': round(t_off, 4),\n",
    "                't_landung': round(t_on, 4),\n",
    "                'flugzeit_s': round(flug_dauer, 4),\n",
    "                'sprunghoehe_cm': round(sprunghoehe_cm, 2)\n",
    "            }\n",
    "            jumps_list.append(jump_dict)\n",
    "            \n",
    "    return jumps_list\n",
    "\n",
    "def visualize_jumps(df, jumps_list, participant, measurement):\n",
    "    # Kraft berechnen\n",
    "    total_force = df['LT Force (N)'] + df['RT Force (N)']\n",
    "    time = df['time'].values\n",
    "    \n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.plot(time, total_force, color='black', alpha=0.3, label='Rohdaten Kraft')\n",
    "    \n",
    "    # Hilfsvariablen f√ºr die Legende\n",
    "    labeled_flight_zone = False\n",
    "    labeled_events = False\n",
    "    \n",
    "    for j in jumps_list:\n",
    "        # 1. Die Flugphase (t_absprung bis t_landung) markieren\n",
    "        plt.axvspan(j['t_absprung'], j['t_landung'], \n",
    "                    color='orange', alpha=0.2, \n",
    "                    label='Flugphase' if not labeled_flight_zone else \"\")\n",
    "        labeled_flight_zone = True\n",
    "        \n",
    "        # 2. Vertikale Linien f√ºr Take-off und Landing\n",
    "        plt.axvline(j['t_absprung'], color='green', linestyle='--', alpha=0.6, \n",
    "                    label='Take-off' if not labeled_events else \"\")\n",
    "        plt.axvline(j['t_landung'], color='red', linestyle='--', alpha=0.6, \n",
    "                    label='Landing' if not labeled_events else \"\")\n",
    "        labeled_events = True\n",
    "        \n",
    "        # 3. Text-Position (Mitte der Flugphase)\n",
    "        text_pos_x = j['t_absprung'] + (j['flugzeit_s'] / 2)\n",
    "        \n",
    "        # 4. Sprungnummer und Sprungh√∂he anzeigen\n",
    "        # Wir platzieren den Text etwas √ºber der Kraftkurve\n",
    "        y_pos = max(total_force) * 0.9\n",
    "        plt.text(text_pos_x, y_pos, f\"Sprung {j['sprung nr.']}\\n{j['sprunghoehe_cm']} cm\", \n",
    "                 horizontalalignment='center', fontweight='bold', fontsize=10,\n",
    "                 bbox=dict(facecolor='white', alpha=0.8, edgecolor='orange', boxstyle='round,pad=0.5'))\n",
    "        \n",
    "        # 5. Flugzeit darunter anzeigen\n",
    "        plt.text(text_pos_x, y_pos * 0.82, f\"t: {j['flugzeit_s']}s\", \n",
    "                 horizontalalignment='center', fontsize=9, color='black', alpha=0.7)\n",
    "\n",
    "    # Titel und Labels\n",
    "    title_str = f\"Sprungdetektion: {participant} - {measurement} | {len(jumps_list)} Spr√ºnge gefunden\"\n",
    "    plt.title(title_str, fontsize=14, pad=15)\n",
    "    plt.xlabel(\"Zeit (s)\")\n",
    "    plt.ylabel(\"Gesamtkraft (N)\")\n",
    "    \n",
    "    # Verzeichnis erstellen (falls nicht vorhanden)\n",
    "    save_dir = 'Pictures_Test/Sprungzyklen_Kraft'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True, alpha=0.2)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Speichern\n",
    "    #file_name = f'{participant}_{measurement}_jump_analysis.png'\n",
    "    #plt.savefig(os.path.join(save_dir, file_name), dpi=300)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "output_folder = 'jump_analysis_results_Pressure'\n",
    "\n",
    "# Erstelle Ausgabeverzeichnis, falls nicht vorhanden\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for participant in participants:\n",
    "    print(f\"\\nVerarbeite {participant}...\")\n",
    "    \n",
    "    all_jumps_data = []\n",
    "    \n",
    "    # Durchlaufe alle 4 Messungen\n",
    "    for measurement in measurements:\n",
    "        df = all_data[participant][measurement]\n",
    "        \n",
    "        # Identifiziere Spr√ºnge in dieser Messung\n",
    "        jumps = identify_jumps(df, flight_threshold=155, min_flight_sec=0.2)\n",
    "        #visualize_jumps(df, jumps, participant, measurement)\n",
    "\n",
    "        # F√ºge Measurement-Info zu jedem Sprung hinzu\n",
    "        for jump in jumps:\n",
    "            jump['messung'] = measurement\n",
    "            all_jumps_data.append(jump)\n",
    "        \n",
    "        print(f\"  {measurement}: {len(jumps)} Spr√ºnge gefunden\")\n",
    "    \n",
    "    # Erstelle DataFrame aus allen Spr√ºngen\n",
    "    jumps_df = pd.DataFrame(all_jumps_data)\n",
    "    \n",
    "    # Speichere als CSV\n",
    "    output_file = os.path.join(output_folder, f'{participant}_jumps.csv')\n",
    "    jumps_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"  ‚úì Gesamt: {len(all_jumps_data)} Spr√ºnge\")\n",
    "    print(f\"  ‚úì CSV gespeichert: {output_file}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Verarbeitung abgeschlossen!\")\n",
    "print(f\"Alle CSVs wurden im Ordner '{output_folder}' gespeichert.\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0052f970",
   "metadata": {},
   "source": [
    "### spr√ºnge nach der H√∂he sortieren und filtern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d18b2192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ KDE-Plots mit Info (n & Mean) erstellt in: Pictures_Test/Jump_Height_Distributions\n",
      "‚úÖ KDE-Plots mit Info (n & Mean) erstellt in: Pictures_Test/Jump_Height_Distributions\n",
      "‚úÖ Dot-Plots mit Info (n & Mean) erstellt in: Pictures_Test/Jump_Height_Dots\n",
      "‚úÖ Dot-Plots mit Info (n & Mean) erstellt in: Pictures_Test/Jump_Height_Dots\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_jump_height_kde_fixed_axes(folder_path, output_name):\n",
    "    \"\"\"\n",
    "    Erstellt KDE-Verteilungskurven f√ºr REAL vs VR mit festen Achsen\n",
    "    f√ºr eine perfekte Vergleichbarkeit zwischen allen Teilnehmern.\n",
    "    \"\"\"\n",
    "    output_dir = 'Pictures_Test/Jump_Height_Distributions'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Suche alle CSV-Dateien im Ordner\n",
    "    csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "    \n",
    "    # --- KONSTANTE ACHSENEINSTELLUNGEN ---\n",
    "    # Diese Werte bleiben f√ºr JEDEN Plot gleich:\n",
    "    X_MIN, X_MAX = 10, 40   # Bereich der Sprungh√∂he in cm\n",
    "    Y_MIN, Y_MAX = 0, 0.3   # Dichte (Wie hoch die Kurve maximal geht)\n",
    "    \n",
    "    for file in csv_files:\n",
    "        file_name = os.path.basename(file)\n",
    "        participant_id = file_name.replace('_jumps.csv', '')\n",
    "        \n",
    "        # Daten laden\n",
    "        df = pd.read_csv(file)\n",
    "        df.columns = df.columns.str.strip() # Entfernt versteckte Leerzeichen\n",
    "        \n",
    "        # REAL und VR trennen\n",
    "        real_heights = df[df['messung'].str.contains('REAL', na=False)]['sprunghoehe_cm'].dropna()\n",
    "        vr_heights = df[df['messung'].str.contains('VR', na=False)]['sprunghoehe_cm'].dropna()\n",
    "        \n",
    "        # Plot initialisieren\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        ax = plt.gca()\n",
    "        \n",
    "        # 1. KDE Kurve f√ºr REAL (Blau)\n",
    "        if not real_heights.empty:\n",
    "            sns.kdeplot(real_heights, fill=True, color=\"blue\", \n",
    "                        label=f\"REAL (n={len(real_heights)})\", ax=ax, alpha=0.3, bw_adjust=1)\n",
    "            # Gestrichelte Linie f√ºr den Mittelwert\n",
    "            ax.axvline(real_heights.mean(), color='blue', linestyle='--', lw=1.5, alpha=0.7)\n",
    "            \n",
    "        # 2. KDE Kurve f√ºr VR (Rot)\n",
    "        if not vr_heights.empty:\n",
    "            sns.kdeplot(vr_heights, fill=True, color=\"red\", \n",
    "                        label=f\"VR (n={len(vr_heights)})\", ax=ax, alpha=0.3, bw_adjust=1)\n",
    "            # Gestrichelte Linie f√ºr den Mittelwert\n",
    "            ax.axvline(vr_heights.mean(), color='red', linestyle='--', lw=1.5, alpha=0.7)\n",
    "        \n",
    "        # --- FIXIERUNG DER ACHSEN ---\n",
    "        ax.set_xlim(X_MIN, X_MAX)\n",
    "        ax.set_ylim(Y_MIN, Y_MAX)\n",
    "        \n",
    "        # Beschriftung und Design\n",
    "        plt.title(f\"Sprungh√∂hen Verteilung: {participant_id}\\n(Fixe Achsen f√ºr Vergleichbarkeit)\", fontsize=13)\n",
    "        plt.xlabel(\"Sprungh√∂he (cm)\", fontsize=12)\n",
    "        plt.ylabel(\"Dichte (Wahrscheinlichkeit)\", fontsize=12)\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.grid(True, linestyle=':', alpha=0.5)\n",
    "        \n",
    "        # Speichern\n",
    "        save_path = os.path.join(output_dir, f\"{participant_id}_{output_name}.png\")\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "    print(f\"‚úÖ KDE-Plots mit Info (n & Mean) erstellt in: {output_dir}\")\n",
    "\n",
    "def plot_jump_height_stripplot_with_info(folder_path, output_name_suffix):\n",
    "    \"\"\"\n",
    "    Erstellt Dot-Plots auf einem Strahl und zeigt die Anzahl der Spr√ºnge (n)\n",
    "    sowie den Mittelwert direkt im Plot an.\n",
    "    \"\"\"\n",
    "    output_dir = 'Pictures_Test/Jump_Height_Dots'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "    \n",
    "    # Achsen fixieren\n",
    "    X_MIN, X_MAX = 10, 45\n",
    "    \n",
    "    for file in csv_files:\n",
    "        file_name = os.path.basename(file)\n",
    "        participant_id = file_name.replace('_jumps.csv', '')\n",
    "        \n",
    "        df = pd.read_csv(file)\n",
    "        df.columns = df.columns.str.strip()\n",
    "        \n",
    "        plt.figure(figsize=(12, 5))\n",
    "        ax = plt.gca()\n",
    "        \n",
    "        for i, (condition, color) in enumerate([('REAL', 'blue'), ('VR', 'red')]):\n",
    "            # Y-Position f√ºr REAL = 1, f√ºr VR = 0\n",
    "            y_pos = 1 if condition == 'REAL' else 0\n",
    "            \n",
    "            data = df[df['messung'].str.contains(condition, na=False)]['sprunghoehe_cm'].dropna()\n",
    "            n_count = len(data)\n",
    "            \n",
    "            if n_count > 0:\n",
    "                mean_val = data.mean()\n",
    "                \n",
    "                # Den Strahl zeichnen\n",
    "                plt.hlines(y_pos, X_MIN, X_MAX, colors='gray', linestyles='--', alpha=0.2)\n",
    "                \n",
    "                # Die einzelnen Spr√ºnge als Punkte\n",
    "                plt.scatter(data, [y_pos]*n_count, color=color, s=120, alpha=0.5, \n",
    "                            edgecolors='white', label=f'{condition} (n={n_count})')\n",
    "                \n",
    "                # Mittelwert-Strich\n",
    "                plt.vlines(mean_val, y_pos - 0.2, y_pos + 0.2, colors='black', lw=3)\n",
    "                \n",
    "                # INFO-TEXT: Anzahl und Mittelwert direkt an den Strich schreiben\n",
    "                plt.text(mean_val, y_pos + 0.25, f'M = {mean_val:.1f} cm\\n(n={n_count})', \n",
    "                         ha='center', va='bottom', fontsize=10, fontweight='bold', color=color)\n",
    "\n",
    "        # Styling\n",
    "        plt.ylim(-0.8, 1.8)\n",
    "        plt.xlim(X_MIN, X_MAX)\n",
    "        plt.yticks([0, 1], ['VR Bedingung', 'REAL Bedingung'], fontsize=12)\n",
    "        plt.title(f\"Einzelwerte & Mittelwerte: {participant_id}\\nDatensatz: {output_name_suffix}\", fontsize=14, pad=20)\n",
    "        plt.xlabel(\"Sprungh√∂he (cm)\", fontsize=12)\n",
    "        plt.grid(axis='x', alpha=0.3, linestyle=':')\n",
    "        \n",
    "        # Speichern\n",
    "        save_path = os.path.join(output_dir, f\"{participant_id}_{output_name_suffix}_dots.png\")\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    print(f\"‚úÖ Dot-Plots mit Info (n & Mean) erstellt in: {output_dir}\")\n",
    "\n",
    "# Aufruf der Funktion (Pfad anpassen)\n",
    "plot_jump_height_kde_fixed_axes('jump_analysis_results_Pressure_angepasst_selber/', \"Distributions_alle\")\n",
    "plot_jump_height_kde_fixed_axes('jump_analysis_results_SD_CLEANED/', \"Distributions_outliers_removed\")\n",
    "\n",
    "plot_jump_height_stripplot_with_info('jump_analysis_results_Pressure_angepasst_selber/', \"Dotplot_alle\")\n",
    "plot_jump_height_stripplot_with_info('jump_analysis_results_SD_CLEANED/', \"Dotplot_outliers_removed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea455dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jump_analysis_results_Pressure_angepasst_selber\\\\ID_1_Dabisch_Samuel_jumps.csv', 'jump_analysis_results_Pressure_angepasst_selber\\\\ID_2_Pohl_Jannis_jumps.csv', 'jump_analysis_results_Pressure_angepasst_selber\\\\ID_3_Kleber_Christian_jumps.csv', 'jump_analysis_results_Pressure_angepasst_selber\\\\ID_4_Schr√∂ter_Till_jumps.csv', 'jump_analysis_results_Pressure_angepasst_selber\\\\ID_5_Zaschke_Lenard_jumps.csv', 'jump_analysis_results_Pressure_angepasst_selber\\\\ID_6_Petroll_Finn_jumps.csv', 'jump_analysis_results_Pressure_angepasst_selber\\\\ID_7_Gruber_Julius_jumps.csv']\n",
      "‚úÖ Check abgeschlossen. Ergebnisse in 'Normalverteilung_Check.csv' gespeichert.\n",
      "               Teilnehmer Bedingung  p-Wert  Normalverteilt?\n",
      "0     ID_1_Dabisch_Samuel      REAL  0.6898               Ja\n",
      "1     ID_1_Dabisch_Samuel        VR  0.3823               Ja\n",
      "2        ID_2_Pohl_Jannis      REAL  0.5446               Ja\n",
      "3        ID_2_Pohl_Jannis        VR  0.0617               Ja\n",
      "4   ID_3_Kleber_Christian      REAL  0.4318               Ja\n",
      "5   ID_3_Kleber_Christian        VR  0.1150               Ja\n",
      "6      ID_4_Schr√∂ter_Till      REAL  0.5018               Ja\n",
      "7      ID_4_Schr√∂ter_Till        VR  0.3847               Ja\n",
      "8     ID_5_Zaschke_Lenard      REAL  0.3145               Ja\n",
      "9     ID_5_Zaschke_Lenard        VR  0.0169  Nein (p < 0.05)\n",
      "10      ID_6_Petroll_Finn      REAL  0.3058               Ja\n",
      "11      ID_6_Petroll_Finn        VR  0.3943               Ja\n",
      "12     ID_7_Gruber_Julius      REAL  0.4133               Ja\n",
      "13     ID_7_Gruber_Julius        VR  0.9926               Ja\n"
     ]
    }
   ],
   "source": [
    "### normalverteilung checken\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "def check_normality(folder_path):\n",
    "    csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "    print (csv_files)\n",
    "    results = []\n",
    "\n",
    "    \n",
    "    for file, participant  in zip(csv_files, participants):\n",
    "        participant_id = participant\n",
    "        df = pd.read_csv(file)\n",
    "        df.columns = df.columns.str.strip()\n",
    "\n",
    "\n",
    "        for cond in ['REAL', 'VR']:\n",
    "            data = df[df['messung'].str.contains(cond, na=False)]['sprunghoehe_cm'].dropna()\n",
    "            \n",
    "            if len(data) >= 3: # Shapiro braucht mind. 3 Werte\n",
    "                stat, p_value = shapiro(data)\n",
    "                is_normal = \"Ja\" if p_value > 0.05 else \"Nein (p < 0.05)\"\n",
    "                \n",
    "                results.append({\n",
    "                    'Teilnehmer': participant_id,\n",
    "                    'Bedingung': cond,\n",
    "                    'p-Wert': round(p_value, 4),\n",
    "                    'Normalverteilt?': is_normal\n",
    "                })\n",
    "\n",
    "    # Ergebnisse als Tabelle speichern\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv('Normalverteilung_Check.csv', index=False)\n",
    "    print(\"‚úÖ Check abgeschlossen. Ergebnisse in 'Normalverteilung_Check.csv' gespeichert.\")\n",
    "    return results_df\n",
    "\n",
    "# Ausf√ºhrung\n",
    "normality_table = check_normality('jump_analysis_results_Pressure_angepasst_selber/')\n",
    "print(normality_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e8dec8",
   "metadata": {},
   "source": [
    "### outlier detektion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17d67511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ SD-Bereinigung abgeschlossen. Dateien in: jump_analysis_results_SD_CLEANED/\n",
      "‚úÇÔ∏è Manuelle Korrektur erfolgreich: Till Schr√∂ter, REAL_2, Sprung 5 wurde entfernt.\n",
      "   Anzahl Spr√ºnge vorher: 18 -> nachher: 17\n",
      "['jump_analysis_results_SD_CLEANED\\\\ID_1_Dabisch_Samuel_jumps.csv', 'jump_analysis_results_SD_CLEANED\\\\ID_2_Pohl_Jannis_jumps.csv', 'jump_analysis_results_SD_CLEANED\\\\ID_3_Kleber_Christian_jumps.csv', 'jump_analysis_results_SD_CLEANED\\\\ID_4_Schr√∂ter_Till_jumps.csv', 'jump_analysis_results_SD_CLEANED\\\\ID_5_Zaschke_Lenard_jumps.csv', 'jump_analysis_results_SD_CLEANED\\\\ID_6_Petroll_Finn_jumps.csv', 'jump_analysis_results_SD_CLEANED\\\\ID_7_Gruber_Julius_jumps.csv']\n",
      "‚úÖ Check abgeschlossen. Ergebnisse in 'Normalverteilung_Check.csv' gespeichert.\n",
      "               Teilnehmer Bedingung  p-Wert Normalverteilt?\n",
      "0     ID_1_Dabisch_Samuel      REAL  0.4220              Ja\n",
      "1     ID_1_Dabisch_Samuel        VR  0.6305              Ja\n",
      "2        ID_2_Pohl_Jannis      REAL  0.2877              Ja\n",
      "3        ID_2_Pohl_Jannis        VR  0.3914              Ja\n",
      "4   ID_3_Kleber_Christian      REAL  0.8062              Ja\n",
      "5   ID_3_Kleber_Christian        VR  0.4627              Ja\n",
      "6      ID_4_Schr√∂ter_Till      REAL  0.6432              Ja\n",
      "7      ID_4_Schr√∂ter_Till        VR  0.1343              Ja\n",
      "8     ID_5_Zaschke_Lenard      REAL  0.2868              Ja\n",
      "9     ID_5_Zaschke_Lenard        VR  0.4208              Ja\n",
      "10      ID_6_Petroll_Finn      REAL  0.1736              Ja\n",
      "11      ID_6_Petroll_Finn        VR  0.7015              Ja\n",
      "12     ID_7_Gruber_Julius      REAL  0.3458              Ja\n",
      "13     ID_7_Gruber_Julius        VR  0.7481              Ja\n"
     ]
    }
   ],
   "source": [
    "### outlier rausschmei√üen\n",
    "def clean_jumps_with_iqr(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Liest Sprungdaten, entfernt Outlier pro Teilnehmer und Bedingung \n",
    "    mittels IQR und speichert die sauberen Daten.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        \n",
    "    csv_files = glob.glob(os.path.join(input_folder, \"*.csv\"))\n",
    "    \n",
    "    summary_report = []\n",
    "\n",
    "    for file_path in csv_files:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        df = pd.read_csv(file_path)\n",
    "        df.columns = df.columns.str.strip()\n",
    "        \n",
    "        cleaned_df_list = []\n",
    "        removed_count_total = 0\n",
    "        \n",
    "        # WICHTIG: Outlier getrennt nach REAL und VR bestimmen\n",
    "        # Sonst l√∂schen wir echte Unterschiede zwischen den Bedingungen!\n",
    "        for condition in ['REAL', 'VR']:\n",
    "            # Filtere Daten f√ºr die aktuelle Bedingung (z.B. alles was 'REAL' enth√§lt)\n",
    "            cond_data = df[df['messung'].str.contains(condition, na=False)].copy()\n",
    "            \n",
    "            if len(cond_data) > 0:\n",
    "                # IQR Berechnung\n",
    "                Q1 = cond_data['sprunghoehe_cm'].quantile(0.25)\n",
    "                Q3 = cond_data['sprunghoehe_cm'].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                \n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                \n",
    "                # Nur Daten behalten, die innerhalb der Bounds liegen\n",
    "                is_no_outlier = (cond_data['sprunghoehe_cm'] >= lower_bound) & \\\n",
    "                                (cond_data['sprunghoehe_cm'] <= upper_bound)\n",
    "                \n",
    "                cond_cleaned = cond_data[is_no_outlier]\n",
    "                removed_in_cond = len(cond_data) - len(cond_cleaned)\n",
    "                \n",
    "                cleaned_df_list.append(cond_cleaned)\n",
    "                removed_count_total += removed_in_cond\n",
    "                \n",
    "                if removed_in_cond > 0:\n",
    "                    print(f\"  - {file_name}: {removed_in_cond} Outlier in {condition} entfernt.\")\n",
    "\n",
    "        # Alle bereinigten Teile wieder zusammenf√ºgen\n",
    "        if cleaned_df_list:\n",
    "            final_df = pd.concat(cleaned_df_list).sort_values(by=['messung', 'sprung nr.'])\n",
    "            final_df.to_csv(os.path.join(output_folder, file_name), index=False)\n",
    "            summary_report.append({'Datei': file_name, 'Entfernt': removed_count_total})\n",
    "\n",
    "    print(f\"\\n‚úÖ Bereinigung abgeschlossen. Daten gespeichert in: {output_folder}\")\n",
    "    return summary_report\n",
    "\n",
    "def clean_jumps_with_iqr_narrow(input_folder, output_folder, iqr_factor=1.0):\n",
    "    \"\"\"\n",
    "    Liest Sprungdaten und entfernt Outlier mit einem engeren IQR-Fenster.\n",
    "    iqr_factor=1.5 ist Standard.\n",
    "    iqr_factor=1.0 ist strenger.\n",
    "    iqr_factor=0.8 ist sehr streng.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        \n",
    "    csv_files = glob.glob(os.path.join(input_folder, \"*.csv\"))\n",
    "    summary_report = []\n",
    "\n",
    "    for file_path in csv_files:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        df = pd.read_csv(file_path)\n",
    "        df.columns = df.columns.str.strip()\n",
    "        \n",
    "        cleaned_df_list = []\n",
    "        removed_count_total = 0\n",
    "        \n",
    "        for condition in ['REAL', 'VR']:\n",
    "            cond_data = df[df['messung'].str.contains(condition, na=False)].copy()\n",
    "            \n",
    "            if len(cond_data) > 0:\n",
    "                # Quartile berechnen\n",
    "                Q1 = cond_data['sprunghoehe_cm'].quantile(0.25)\n",
    "                Q3 = cond_data['sprunghoehe_cm'].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                \n",
    "                # FENSTER VERKLEINERN: Hier nutzen wir den kleineren iqr_factor\n",
    "                lower_bound = Q1 - iqr_factor * IQR\n",
    "                upper_bound = Q3 + iqr_factor * IQR\n",
    "                \n",
    "                # Filter anwenden\n",
    "                mask = (cond_data['sprunghoehe_cm'] >= lower_bound) & \\\n",
    "                       (cond_data['sprunghoehe_cm'] <= upper_bound)\n",
    "                \n",
    "                cond_cleaned = cond_data[mask]\n",
    "                removed_in_cond = len(cond_data) - len(cond_cleaned)\n",
    "                \n",
    "                cleaned_df_list.append(cond_cleaned)\n",
    "                removed_count_total += removed_in_cond\n",
    "                \n",
    "                if removed_in_cond > 0:\n",
    "                    print(f\"üö© {file_name}: {removed_in_cond} Outlier in {condition} entfernt.\")\n",
    "                    print(f\"   Grenzen (IQR x {iqr_factor}): {lower_bound:.2f} - {upper_bound:.2f} cm\")\n",
    "\n",
    "        if cleaned_df_list:\n",
    "            final_df = pd.concat(cleaned_df_list).sort_values(by=['messung', 'sprung nr.'])\n",
    "            final_df.to_csv(os.path.join(output_folder, file_name), index=False)\n",
    "            summary_report.append({'Datei': file_name, 'Entfernt': removed_count_total})\n",
    "\n",
    "    print(f\"\\n‚úÖ Enge IQR-Bereinigung abgeschlossen. Ordner: {output_folder}\")\n",
    "    return summary_report\n",
    "\n",
    "# # --- AUSF√úHRUNG MIT ENGEREM FENSTER ---\n",
    "# # Wir probieren es mal mit 1.0 statt 1.5\n",
    "# clean_jumps_with_iqr_narrow('jump_analysis_results_Pressure_angepasst/', \n",
    "#                            'jump_analysis_results_IQR_CLEANED/', \n",
    "#                            iqr_factor=0.8)\n",
    "\n",
    "# # --- AUSF√úHRUNG ---\n",
    "# input_path = 'jump_analysis_results_Pressure_angepasst/'\n",
    "# output_path = 'jump_analysis_results_CLEANED/'\n",
    "\n",
    "# report = clean_jumps_with_iqr(input_path, output_path)\n",
    "\n",
    "\n",
    "def clean_jumps_with_sd(input_folder, output_folder, sd_factor=2):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        \n",
    "    csv_files = glob.glob(os.path.join(input_folder, \"*.csv\"))\n",
    "    \n",
    "    for file_path in csv_files:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        df = pd.read_csv(file_path)\n",
    "        df.columns = df.columns.str.strip()\n",
    "        \n",
    "        cleaned_chunks = []\n",
    "        \n",
    "        for condition in ['REAL', 'VR']:\n",
    "            # Filter f√ºr die Bedingung\n",
    "            cond_data = df[df['messung'].str.contains(condition, na=False)].copy()\n",
    "            \n",
    "            if len(cond_data) > 2: # SD ist bei n=2 nicht aussagekr√§ftig\n",
    "                mean_h = cond_data['sprunghoehe_cm'].mean()\n",
    "                std_h = cond_data['sprunghoehe_cm'].std()\n",
    "                \n",
    "                lower_limit = mean_h - (sd_factor * std_h)\n",
    "                upper_limit = mean_h + (sd_factor * std_h)\n",
    "                \n",
    "                # Maske erstellen (Korrigiert: Variable konsistent benannt)\n",
    "                mask = (cond_data['sprunghoehe_cm'] >= lower_limit) & \\\n",
    "                       (cond_data['sprunghoehe_cm'] <= upper_limit)\n",
    "                \n",
    "                cond_cleaned = cond_data[mask]\n",
    "                removed = len(cond_data) - len(cond_cleaned)\n",
    "                \n",
    "                if removed > 0:\n",
    "                    print(f\"üö© {file_name} ({condition}): {removed} Outlier entfernt.\")\n",
    "                    print(f\"   Grenzen: {lower_limit:.2f} - {upper_limit:.2f} cm (Mean: {mean_h:.2f})\")\n",
    "                \n",
    "                cleaned_chunks.append(cond_cleaned)\n",
    "            else:\n",
    "                cleaned_chunks.append(cond_data)\n",
    "\n",
    "        if cleaned_chunks:\n",
    "            final_df = pd.concat(cleaned_chunks)\n",
    "            # Sortierung sicherstellen\n",
    "            if 'sprung nr.' in final_df.columns:\n",
    "                final_df = final_df.sort_values(by=['messung', 'sprung nr.'])\n",
    "            \n",
    "            final_df.to_csv(os.path.join(output_folder, file_name), index=False)\n",
    "\n",
    "    print(f\"\\n‚úÖ SD-Bereinigung abgeschlossen. Dateien in: {output_folder}\")\n",
    "\n",
    "# Ausf√ºhrung\n",
    "clean_jumps_with_sd('jump_analysis_results_Pressure_angepasst/', \n",
    "                    'jump_analysis_results_SD_CLEANED/', \n",
    "                    sd_factor=1.2) # Strenger!\n",
    "\n",
    "# 2. MANUELLE KORREKTUR: Till Schr√∂ter Real 2 Sprung 5 entfernen\n",
    "file_path = 'jump_analysis_results_SD_CLEANED/ID_4_Schr√∂ter_Till_jumps.csv'\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    # Datei einladen\n",
    "    df_manual = pd.read_csv(file_path)\n",
    "    \n",
    "    # Zeile finden und entfernen\n",
    "    # Wir behalten alle Zeilen, au√üer die Kombination aus REAL_2 und Sprung 5\n",
    "    df_filtered = df_manual[~((df_manual['messung'] == 'REAL_2') & (df_manual['sprung nr.'] == 5))]\n",
    "    \n",
    "    # Speichern (√ºberschreibt die Datei im CLEANED-Ordner)\n",
    "    df_filtered.to_csv(file_path, index=False)\n",
    "    \n",
    "    print(f\"‚úÇÔ∏è Manuelle Korrektur erfolgreich: Till Schr√∂ter, REAL_2, Sprung 5 wurde entfernt.\")\n",
    "    print(f\"   Anzahl Spr√ºnge vorher: {len(df_manual)} -> nachher: {len(df_filtered)}\")\n",
    "\n",
    "\n",
    "# normalverteilung nach outlier entfernung checken\n",
    "normality_table = check_normality('jump_analysis_results_SD_CLEANED/')\n",
    "print(normality_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55e4163",
   "metadata": {},
   "source": [
    "## hier werden der takeoff zeitpunkt anhand des Kniegelenks bestimmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47bcd0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verarbeite ID_1_Dabisch_Samuel...\n",
      "  REAL_1: 6 Spr√ºnge gefunden\n",
      "  REAL_2: 6 Spr√ºnge gefunden\n",
      "  VR_1: 6 Spr√ºnge gefunden\n",
      "  VR_2: 6 Spr√ºnge gefunden\n",
      "  ‚úì Gesamt: 24 Spr√ºnge\n",
      "  ‚úì CSV gespeichert: jump_analysis_results_knee_alle\\ID_1_Dabisch_Samuel_jumps.csv\n",
      "\n",
      "Verarbeite ID_2_Pohl_Jannis...\n",
      "  REAL_1: 6 Spr√ºnge gefunden\n",
      "  REAL_2: 5 Spr√ºnge gefunden\n",
      "  VR_1: 6 Spr√ºnge gefunden\n",
      "  VR_2: 1 Spr√ºnge gefunden\n",
      "  ‚úì Gesamt: 18 Spr√ºnge\n",
      "  ‚úì CSV gespeichert: jump_analysis_results_knee_alle\\ID_2_Pohl_Jannis_jumps.csv\n",
      "\n",
      "Verarbeite ID_3_Kleber_Christian...\n",
      "  REAL_1: 6 Spr√ºnge gefunden\n",
      "  REAL_2: 6 Spr√ºnge gefunden\n",
      "  VR_1: 6 Spr√ºnge gefunden\n",
      "  VR_2: 6 Spr√ºnge gefunden\n",
      "  ‚úì Gesamt: 24 Spr√ºnge\n",
      "  ‚úì CSV gespeichert: jump_analysis_results_knee_alle\\ID_3_Kleber_Christian_jumps.csv\n",
      "\n",
      "Verarbeite ID_4_Schr√∂ter_Till...\n",
      "  REAL_1: 6 Spr√ºnge gefunden\n",
      "  REAL_2: 6 Spr√ºnge gefunden\n",
      "  VR_1: 5 Spr√ºnge gefunden\n",
      "  VR_2: 6 Spr√ºnge gefunden\n",
      "  ‚úì Gesamt: 23 Spr√ºnge\n",
      "  ‚úì CSV gespeichert: jump_analysis_results_knee_alle\\ID_4_Schr√∂ter_Till_jumps.csv\n",
      "\n",
      "Verarbeite ID_5_Zaschke_Lenard...\n",
      "  REAL_1: 6 Spr√ºnge gefunden\n",
      "  REAL_2: 6 Spr√ºnge gefunden\n",
      "  VR_1: 6 Spr√ºnge gefunden\n",
      "  VR_2: 6 Spr√ºnge gefunden\n",
      "  ‚úì Gesamt: 24 Spr√ºnge\n",
      "  ‚úì CSV gespeichert: jump_analysis_results_knee_alle\\ID_5_Zaschke_Lenard_jumps.csv\n",
      "\n",
      "Verarbeite ID_6_Petroll_Finn...\n",
      "  REAL_1: 6 Spr√ºnge gefunden\n",
      "  REAL_2: 6 Spr√ºnge gefunden\n",
      "  VR_1: 6 Spr√ºnge gefunden\n",
      "  VR_2: 6 Spr√ºnge gefunden\n",
      "  ‚úì Gesamt: 24 Spr√ºnge\n",
      "  ‚úì CSV gespeichert: jump_analysis_results_knee_alle\\ID_6_Petroll_Finn_jumps.csv\n",
      "\n",
      "Verarbeite ID_7_Gruber_Julius...\n",
      "  REAL_1: 6 Spr√ºnge gefunden\n",
      "  REAL_2: 6 Spr√ºnge gefunden\n",
      "  VR_1: 6 Spr√ºnge gefunden\n",
      "  VR_2: 6 Spr√ºnge gefunden\n",
      "  ‚úì Gesamt: 24 Spr√ºnge\n",
      "  ‚úì CSV gespeichert: jump_analysis_results_knee_alle\\ID_7_Gruber_Julius_jumps.csv\n",
      "\n",
      "================================================================================\n",
      "Verarbeitung abgeschlossen!\n",
      "Alle CSVs wurden im Ordner 'jump_analysis_results_knee_alle' gespeichert.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "### takeofoff mit kniewinkel\n",
    "def identify_jumps_by_knee(df, angle_col='RT Knee Flexion (deg)', min_jump_distance_sec=5.0, threshold_angle=60, buffer=0.75, fs=2000):\n",
    "    \"\"\"\n",
    "    Identifiziert Spr√ºnge sequentiell ohne Filterung:\n",
    "    Sucht nach dem Peak > 60¬∞ und nimmt das ALLERERSTE lokale Minimum danach als Takeoff.\n",
    "    \"\"\"\n",
    "    angle = df[angle_col].values\n",
    "    time = df['time'].values\n",
    "    \n",
    "    jumps_list = []\n",
    "    i = 0\n",
    "    pause_points = int(min_jump_distance_sec * fs)\n",
    "    search_window_points = int(1.5 * fs)\n",
    "\n",
    "    while i < len(angle):\n",
    "        # A. Suche nach dem n√§chsten Punkt √ºber dem Schwellenwert (Ausholbewegung)\n",
    "        if angle[i] >= threshold_angle:\n",
    "            # 1. Den exakten Peak (maximale Beugung) im Umkreis finden\n",
    "            peak_search_end = min(i + int(0.5 * fs), len(angle))\n",
    "            peak_idx = i + np.argmax(angle[i:peak_search_end])\n",
    "            \n",
    "            # 2. Suche Takeoff: Das ERSTE lokale Minimum nach dem Peak\n",
    "            takeoff_idx = peak_idx # Fallback\n",
    "            \n",
    "            # Wir laufen vom Peak vorw√§rts und suchen den ersten Punkt, \n",
    "            # an dem die Kurve nicht mehr weiter f√§llt.\n",
    "            for j in range(peak_idx + 1, min(peak_idx + search_window_points, len(angle) - 1)):\n",
    "                if angle[j] <= angle[j-1] and angle[j] <= angle[j+1]:\n",
    "                    takeoff_idx = j\n",
    "                    break\n",
    "            else:\n",
    "                # Falls kein lokales Minimum gefunden wurde, Backup: Absolutes Minimum\n",
    "                search_end = min(peak_idx + search_window_points, len(angle))\n",
    "                takeoff_idx = peak_idx + np.argmin(angle[peak_idx:search_end])\n",
    "\n",
    "            # 3. Daten extrahieren\n",
    "            t_absprung = time[takeoff_idx]\n",
    "            start_analyse = max(0, t_absprung - buffer)\n",
    "            \n",
    "            jumps_list.append({\n",
    "                'sprung nr.': len(jumps_list) + 1,\n",
    "                'start_analyse': round(start_analyse, 4),\n",
    "                't_absprung': round(t_absprung, 4)\n",
    "            })\n",
    "            \n",
    "            # 4. Sperrfrist: 5 Sekunden nach dem Takeoff √ºberspringen\n",
    "            i = takeoff_idx + pause_points\n",
    "            continue\n",
    "        \n",
    "        i += 1\n",
    "            \n",
    "    return jumps_list\n",
    "\n",
    "def visualize_knee_jumps(df, jumps_list, participant=\"\", measurement=\"\"):\n",
    "    \"\"\"\n",
    "    Visualisiert die Sprungerkennung basierend auf dem Kniewinkel.\n",
    "    Korrektur: alpha in tick_params entfernt, da nicht unterst√ºtzt.\n",
    "    \"\"\"\n",
    "    time = df['time'].values\n",
    "    # Sicherstellen, dass die Spalte exakt so hei√üt\n",
    "    knee_angle = df['RT Knee Flexion (deg)'].values\n",
    "    total_force = df['LT Force (N)'] + df['RT Force (N)']\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(15, 7))\n",
    "\n",
    "    # Prim√§re Achse: Kniewinkel\n",
    "    ax1.plot(time, knee_angle, color='blue', alpha=0.6, label='Kniewinkel (RT Flexion)')\n",
    "    ax1.set_ylabel('Kniewinkel (deg)', color='blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue') # alpha entfernt\n",
    "\n",
    "    # # Sekund√§re Achse f√ºr die Kraft\n",
    "    # ax2 = ax1.twinx()\n",
    "    # ax2.plot(time, total_force, color='black', alpha=0.2, label='Gesamtkraft (N)')\n",
    "    # ax2.set_ylabel('Kraft (N)', color='gray') \n",
    "    # ax2.tick_params(axis='y', labelcolor='gray') # alpha entfernt\n",
    "\n",
    "    labeled_buffer = False\n",
    "    labeled_takeoff = False\n",
    "\n",
    "    for j in jumps_list:\n",
    "        # 1. Analyse-Fenster (Buffer bis Takeoff)\n",
    "        # Beachte: Die Keys m√ºssen 'start_analyse' und 't_absprung' sein (wie in deiner Funktion)\n",
    "        ax1.axvspan(j['start_analyse'], j['t_absprung'], \n",
    "                    color='gray', alpha=0.15, \n",
    "                    label='Analyse-Fenster (Buffer)' if not labeled_buffer else \"\")\n",
    "        labeled_buffer = True\n",
    "        \n",
    "        # 2. Vertikale Linie f√ºr Takeoff\n",
    "        ax1.axvline(j['t_absprung'], color='red', linestyle='--', lw=2,\n",
    "                    label='Takeoff (Knie-Extremum)' if not labeled_takeoff else \"\")\n",
    "        labeled_takeoff = True\n",
    "        \n",
    "        # 3. Sprungnummer\n",
    "        ax1.text(j['t_absprung'], max(knee_angle) * 0.95, f\"S{j['sprung nr.']}\", \n",
    "                 color='red', fontweight='bold', ha='right')\n",
    "\n",
    "    plt.title(f\"Knie-basierte Sprungerkennung: {participant} - {measurement} ({len(jumps_list)} Spr√ºnge)\")\n",
    "    ax1.set_xlabel(\"Zeit (s)\")\n",
    "    \n",
    "    # Legenden zusammenf√ºhren\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    # lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 , labels1 , loc='upper right')\n",
    "    \n",
    "    ax1.grid(True, alpha=0.2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "output_folder = 'jump_analysis_results_knee_alle'\n",
    "\n",
    "# Erstelle Ausgabeverzeichnis, falls nicht vorhanden\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for participant in participants:\n",
    "    print(f\"\\nVerarbeite {participant}...\")\n",
    "    \n",
    "    all_jumps_data = []\n",
    "    \n",
    "    # Durchlaufe alle 4 Messungen\n",
    "    for measurement in measurements:\n",
    "        df = all_data[participant][measurement]\n",
    "        \n",
    "        # Identifiziere Spr√ºnge in dieser Messung\n",
    "        jumps = identify_jumps_by_knee(df,  min_jump_distance_sec = 5, buffer=0.7)\n",
    "        #visualize_knee_jumps(df, jumps, participant, measurement)\n",
    "\n",
    "        # F√ºge Measurement-Info zu jedem Sprung hinzu\n",
    "        for jump in jumps:\n",
    "            jump['messung'] = measurement\n",
    "            all_jumps_data.append(jump)\n",
    "        \n",
    "        print(f\"  {measurement}: {len(jumps)} Spr√ºnge gefunden\")\n",
    "    \n",
    "    # Erstelle DataFrame aus allen Spr√ºngen\n",
    "    jumps_df = pd.DataFrame(all_jumps_data)\n",
    "    \n",
    "    # Speichere als CSV\n",
    "    output_file = os.path.join(output_folder, f'{participant}_jumps.csv')\n",
    "    jumps_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"  ‚úì Gesamt: {len(all_jumps_data)} Spr√ºnge\")\n",
    "    print(f\"  ‚úì CSV gespeichert: {output_file}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Verarbeitung abgeschlossen!\")\n",
    "print(f\"Alle CSVs wurden im Ordner '{output_folder}' gespeichert.\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504c5c1a",
   "metadata": {},
   "source": [
    "### nur richtige spr√ºnge mit reinnehmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2785644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ID_1_Dabisch_Samuel_jumps.csv: 17 Spr√ºnge synchronisiert (7 Outlier entfernt)\n",
      "‚úÖ ID_2_Pohl_Jannis_jumps.csv: 14 Spr√ºnge synchronisiert (4 Outlier entfernt)\n",
      "‚úÖ ID_3_Kleber_Christian_jumps.csv: 19 Spr√ºnge synchronisiert (5 Outlier entfernt)\n",
      "‚úÖ ID_4_Schr√∂ter_Till_jumps.csv: 17 Spr√ºnge synchronisiert (6 Outlier entfernt)\n",
      "‚úÖ ID_5_Zaschke_Lenard_jumps.csv: 22 Spr√ºnge synchronisiert (2 Outlier entfernt)\n",
      "‚úÖ ID_6_Petroll_Finn_jumps.csv: 17 Spr√ºnge synchronisiert (7 Outlier entfernt)\n",
      "‚úÖ ID_7_Gruber_Julius_jumps.csv: 19 Spr√ºnge synchronisiert (5 Outlier entfernt)\n"
     ]
    }
   ],
   "source": [
    "def synchronize_knee_with_cleaned_jumps(knee_alle_folder, cleaned_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Filtert die Zeitstempel der Knie-Analyse so, dass nur noch die Spr√ºnge \n",
    "    √ºbrig bleiben, die auch die Outlier-Bereinigung (SD) bestanden haben.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        \n",
    "    # Suche alle CSV-Dateien im Quellordner (Knie-Alle)\n",
    "    knee_files = glob.glob(os.path.join(knee_alle_folder, \"*.csv\"))\n",
    "    \n",
    "    for knee_file_path in knee_files:\n",
    "        file_name = os.path.basename(knee_file_path)\n",
    "        cleaned_file_path = os.path.join(cleaned_folder, file_name)\n",
    "        \n",
    "        # Pr√ºfen, ob f√ºr diesen Teilnehmer eine bereinigte Datei existiert\n",
    "        if not os.path.exists(cleaned_file_path):\n",
    "            print(f\"‚ö†Ô∏è Keine bereinigte Datei f√ºr {file_name} gefunden. √úberspringe...\")\n",
    "            continue\n",
    "            \n",
    "        # 1. Daten laden\n",
    "        df_knee = pd.read_csv(knee_file_path)\n",
    "        df_cleaned = pd.read_csv(cleaned_file_path)\n",
    "        \n",
    "        # Spaltennamen s√§ubern\n",
    "        df_knee.columns = df_knee.columns.str.strip()\n",
    "        df_cleaned.columns = df_cleaned.columns.str.strip()\n",
    "        \n",
    "        # 2. Synchronisieren (Inner Join)\n",
    "        # Wir behalten nur Zeilen aus df_knee, deren Kombi aus 'messung' \n",
    "        # und 'sprung nr.' auch in df_cleaned vorkommt.\n",
    "        # Wir nutzen merge, um nur die Spalten von df_knee zu behalten.\n",
    "        df_synced = pd.merge(\n",
    "            df_knee, \n",
    "            df_cleaned[['messung', 'sprung nr.']], \n",
    "            on=['messung', 'sprung nr.'], \n",
    "            how='inner'\n",
    "        )\n",
    "        \n",
    "        # 3. Sortierung beibehalten (Optional, aber sauberer)\n",
    "        df_synced = df_synced.sort_values(by=['messung', 'sprung nr.'])\n",
    "        \n",
    "        # 4. Speichern\n",
    "        output_path = os.path.join(output_folder, file_name)\n",
    "        df_synced.to_csv(output_path, index=False)\n",
    "        \n",
    "        removed = len(df_knee) - len(df_synced)\n",
    "        print(f\"‚úÖ {file_name}: {len(df_synced)} Spr√ºnge synchronisiert ({removed} Outlier entfernt)\")\n",
    "\n",
    "# --- AUSF√úHRUNG ---\n",
    "synchronize_knee_with_cleaned_jumps(\n",
    "    knee_alle_folder='jump_analysis_results_knee_alle', \n",
    "    cleaned_folder='jump_analysis_results_SD_CLEANED', \n",
    "    output_folder='jump_analysis_results_knee'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de04332",
   "metadata": {},
   "source": [
    "## Zeitnormieren auf sekunde vor takeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15ff49f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starte Zeit-Normierung (Knie-Logik)...\n",
      "Normiere Daten f√ºr ID_1_Dabisch_Samuel...\n",
      "  ‚úì REAL_1: 4 Spr√ºnge normiert\n",
      "  ‚úì REAL_2: 4 Spr√ºnge normiert\n",
      "  ‚úì VR_1: 5 Spr√ºnge normiert\n",
      "  ‚úì VR_2: 4 Spr√ºnge normiert\n",
      "17\n",
      "Normiere Daten f√ºr ID_2_Pohl_Jannis...\n",
      "  ‚úì REAL_1: 5 Spr√ºnge normiert\n",
      "  ‚úì REAL_2: 4 Spr√ºnge normiert\n",
      "  ‚úì VR_1: 5 Spr√ºnge normiert\n",
      "  ‚úì VR_2: 0 Spr√ºnge normiert\n",
      "14\n",
      "Normiere Daten f√ºr ID_3_Kleber_Christian...\n",
      "  ‚úì REAL_1: 5 Spr√ºnge normiert\n",
      "  ‚úì REAL_2: 4 Spr√ºnge normiert\n",
      "  ‚úì VR_1: 6 Spr√ºnge normiert\n",
      "  ‚úì VR_2: 4 Spr√ºnge normiert\n",
      "19\n",
      "Normiere Daten f√ºr ID_4_Schr√∂ter_Till...\n",
      "  ‚úì REAL_1: 3 Spr√ºnge normiert\n",
      "  ‚úì REAL_2: 5 Spr√ºnge normiert\n",
      "  ‚úì VR_1: 5 Spr√ºnge normiert\n",
      "  ‚úì VR_2: 4 Spr√ºnge normiert\n",
      "17\n",
      "Normiere Daten f√ºr ID_5_Zaschke_Lenard...\n",
      "  ‚úì REAL_1: 5 Spr√ºnge normiert\n",
      "  ‚úì REAL_2: 6 Spr√ºnge normiert\n",
      "  ‚úì VR_1: 5 Spr√ºnge normiert\n",
      "  ‚úì VR_2: 6 Spr√ºnge normiert\n",
      "22\n",
      "Normiere Daten f√ºr ID_6_Petroll_Finn...\n",
      "  ‚úì REAL_1: 4 Spr√ºnge normiert\n",
      "  ‚úì REAL_2: 5 Spr√ºnge normiert\n",
      "  ‚úì VR_1: 5 Spr√ºnge normiert\n",
      "  ‚úì VR_2: 3 Spr√ºnge normiert\n",
      "17\n",
      "Normiere Daten f√ºr ID_7_Gruber_Julius...\n",
      "  ‚úì REAL_1: 5 Spr√ºnge normiert\n",
      "  ‚úì REAL_2: 5 Spr√ºnge normiert\n",
      "  ‚úì VR_1: 4 Spr√ºnge normiert\n",
      "  ‚úì VR_2: 5 Spr√ºnge normiert\n",
      "19\n",
      "\n",
      "Fertig! Alle Spr√ºnge sind nun zeitnormiert (Buffer -> Take-off) verf√ºgbar.\n",
      "--- Check pro Teilnehmer ---\n",
      "ID_1_Dabisch_Samuel: REAL=8, VR=9\n",
      "ID_2_Pohl_Jannis: REAL=9, VR=5\n",
      "ID_3_Kleber_Christian: REAL=9, VR=10\n",
      "ID_4_Schr√∂ter_Till: REAL=8, VR=9\n",
      "ID_5_Zaschke_Lenard: REAL=11, VR=11\n",
      "ID_6_Petroll_Finn: REAL=9, VR=8\n",
      "ID_7_Gruber_Julius: REAL=10, VR=9\n",
      "\n",
      "GESAMT-STATISTIK:\n",
      "REAL: 64 Spr√ºnge (von urspr√ºnglich 84)\n",
      "VR:   61 Spr√ºnge (von urspr√ºnglich 84)\n"
     ]
    }
   ],
   "source": [
    "### Zeitnormerierung der Spr√ºnge basierend auf Knie-Extraktion\n",
    "def time_normalize_jumps_knee(df, participant_name, measurement_type, \n",
    "                             jumps_csv_folder='jump_analysis_results_knee', \n",
    "                             normalize_points=100):\n",
    "    \"\"\"\n",
    "    Normiert die Spr√ºnge basierend auf der Knie-Extraktion (start_analyse bis t_absprung).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Der Rohdaten-DataFrame (z.B. df_real1).\n",
    "    participant_name : str\n",
    "        Name des Probanden (f√ºr den CSV-Dateinamen).\n",
    "    measurement_type : str\n",
    "        Die aktuelle Messung (z.B. 'REAL_1').\n",
    "    jumps_csv_folder : str\n",
    "        Ordner, in dem die Ergebnisse der Knie-Analyse liegen.\n",
    "    normalize_points : int\n",
    "        Anzahl der Datenpunkte nach der Normierung (Standard: 100).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pfad zur CSV (jetzt aus dem knee-Ordner)\n",
    "    jumps_csv_path = os.path.join(jumps_csv_folder, f'{participant_name}_jumps.csv')\n",
    "    \n",
    "    if not os.path.exists(jumps_csv_path):\n",
    "        print(f\"Warnung: CSV nicht gefunden {jumps_csv_path}\")\n",
    "        return {}\n",
    "    \n",
    "    # 1. CSV laden\n",
    "    jumps_info = pd.read_csv(jumps_csv_path)\n",
    "    \n",
    "    # 2. Filter auf die aktuelle Messung (z.B. nur REAL_1)\n",
    "    relevant_jumps = jumps_info[jumps_info['messung'] == measurement_type]\n",
    "    \n",
    "    normalized_jumps = {}\n",
    "    \n",
    "    # 3. √úber die gefundenen Spr√ºnge iterieren\n",
    "    for idx, row in relevant_jumps.iterrows():\n",
    "        # Zeitgrenzen aus deiner neuen Struktur\n",
    "        start_time = row['start_analyse']\n",
    "        end_time = row['t_absprung']\n",
    "        jump_nr = int(row['sprung nr.'])\n",
    "        \n",
    "        # Daten im Zeitfenster ausschneiden\n",
    "        mask = (df['time'] >= start_time) & (df['time'] <= end_time)\n",
    "        jump_data = df[mask].reset_index(drop=True)\n",
    "        \n",
    "        # Sicherheitscheck falls keine Daten im Fenster sind\n",
    "        if len(jump_data) < 2:\n",
    "            continue\n",
    "            \n",
    "        # --- Zeit-Normierung (Interpolation) ---\n",
    "        # Erstelle Indizes f√ºr die urspr√ºnglichen Daten (z.B. 0 bis 145)\n",
    "        original_indices = np.linspace(0, len(jump_data) - 1, len(jump_data))\n",
    "        # Erstelle Indizes f√ºr die Ziel-Punkte (z.B. 0 bis 99)\n",
    "        new_indices = np.linspace(0, len(jump_data) - 1, normalize_points)\n",
    "        \n",
    "        normalized_data = {}\n",
    "        for col in jump_data.columns:\n",
    "            # Nur numerische Spalten (Kraft, Winkel, EMG) interpolieren\n",
    "            if pd.api.types.is_numeric_dtype(jump_data[col]):\n",
    "                normalized_data[col] = np.interp(new_indices, original_indices, jump_data[col].values)\n",
    "            else:\n",
    "                # Nicht-numerische Spalten (z.B. ID) einfach den ersten Wert beibehalten\n",
    "                normalized_data[col] = [jump_data[col].iloc[0]] * normalize_points\n",
    "        \n",
    "        # F√ºge die normierte Zeitachse (0% bis 100%) hinzu\n",
    "        normalized_data['time_normalized'] = np.linspace(0, 1, normalize_points)\n",
    "        \n",
    "        # In DataFrame umwandeln\n",
    "        normalized_df = pd.DataFrame(normalized_data)\n",
    "        \n",
    "        # Eindeutiger Key f√ºr das Dictionary\n",
    "        key = f\"{measurement_type}_jump_{jump_nr}\"\n",
    "        normalized_jumps[key] = normalized_df\n",
    "    \n",
    "    return normalized_jumps\n",
    "\n",
    "# %% Zeit-Normierung basierend auf Knie-Extraktion\n",
    "print(f\"\\nStarte Zeit-Normierung (Knie-Logik)...\")\n",
    "\n",
    "# Neues Zielverzeichnis f√ºr die Knie-Ergebnisse\n",
    "output_folder_knee = 'jump_analysis_results_knee'\n",
    "# output_folder_knee = 'jump_analysis_results_SD_CLEANED'  # Angepasster Ordnername\n",
    "\n",
    "all_normalized_data = {}\n",
    "\n",
    "for participant in participants:\n",
    "    print(f\"Normiere Daten f√ºr {participant}...\")\n",
    "    all_normalized_data[participant] = {}\n",
    "    \n",
    "    for measurement in measurements:\n",
    "        df = all_data[participant][measurement]\n",
    "        \n",
    "        # Wichtig: Nutze hier die neue Funktion, die 'start_analyse' und 't_absprung' liest\n",
    "        norm_jumps_dict = time_normalize_jumps_knee(\n",
    "            df, \n",
    "            participant, \n",
    "            measurement, \n",
    "            jumps_csv_folder=output_folder_knee, # Ge√§nderter Ordner\n",
    "            normalize_points=100\n",
    "        )\n",
    "        \n",
    "        # Speichern im Haupt-Dictionary\n",
    "        all_normalized_data[participant].update(norm_jumps_dict)\n",
    "        \n",
    "        print(f\"  ‚úì {measurement}: {len(norm_jumps_dict)} Spr√ºnge normiert\")\n",
    "    \n",
    "    print (len(all_normalized_data[participant]))\n",
    "\n",
    "\n",
    "print(f\"\\nFertig! Alle Spr√ºnge sind nun zeitnormiert (Buffer -> Take-off) verf√ºgbar.\")\n",
    "\n",
    "\n",
    "# DIAGNOSE: Wie viele Spr√ºnge sind wirklich im Dictionary?\n",
    "total_real = 0\n",
    "total_vr = 0\n",
    "\n",
    "print(\"--- Check pro Teilnehmer ---\")\n",
    "for p_id in all_normalized_data:\n",
    "    keys = list(all_normalized_data[p_id].keys())\n",
    "    real_count = len([k for k in keys if 'REAL' in k])\n",
    "    vr_count = len([k for k in keys if 'VR' in k])\n",
    "    print(f\"{p_id}: REAL={real_count}, VR={vr_count}\")\n",
    "    total_real += real_count\n",
    "    total_vr += vr_count\n",
    "\n",
    "print(f\"\\nGESAMT-STATISTIK:\")\n",
    "print(f\"REAL: {total_real} Spr√ºnge (von urspr√ºnglich 84)\")\n",
    "print(f\"VR:   {total_vr} Spr√ºnge (von urspr√ºnglich 84)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363c6c95",
   "metadata": {},
   "source": [
    "## Knie, H√ºft und Sprunggelenk kurven pro teilneher und √ºber alle teilnehmer + mittelwert/ standartabweichung "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f929e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plotten und speichern der normalisierten Bedingungen\n",
    "def plot_normalized_conditions(normalized_dict, participant_name, condition_type, column_to_plot):\n",
    "    \"\"\"\n",
    "    Fasst alle Spr√ºnge einer Bedingung (z.B. REAL_1 + REAL_2) zusammen.\n",
    "    Erstellt:\n",
    "    1. Ein gro√ües Subplot-Bild mit allen ~12 Spr√ºngen.\n",
    "    2. Ein Vergleichsbild (Overlaid) mit dem Gesamt-Mittelwert der Bedingung.\n",
    "    \"\"\"\n",
    "    # 1. Vorbereitung Ordnerstruktur\n",
    "    target_dir = os.path.join('Pictures_Test', 'Conditions_Combined', column_to_plot.replace(' ', '_'))\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    \n",
    "    # Filtere die Keys: Sucht nach allen Keys, die den Bedingungs-String enthalten\n",
    "    # z.B. wenn condition_type='REAL', findet er 'REAL_1_jump_1' bis 'REAL_2_jump_6'\n",
    "    jump_keys = [k for k in normalized_dict.keys() if condition_type in k]\n",
    "    num_jumps = len(jump_keys)\n",
    "    \n",
    "    if num_jumps == 0:\n",
    "        print(f\"Keine Daten f√ºr Bedingung {condition_type} bei {participant_name} gefunden.\")\n",
    "        return\n",
    "\n",
    "    # Daten sammeln\n",
    "    all_values = []\n",
    "    time_normalized = None\n",
    "\n",
    "    # --- GRAFIK 1: SUBPLOTS (Alle 12 untereinander) ---\n",
    "    # Wir machen das Layout etwas kompakter, da 12 Spr√ºnge viel Platz brauchen\n",
    "    fig_sub, axes = plt.subplots(num_jumps, 1, figsize=(10, 2 * num_jumps), sharex=True)\n",
    "    if num_jumps == 1: axes = [axes]\n",
    "    \n",
    "    for i, key in enumerate(jump_keys):\n",
    "        df_jump = normalized_dict[key]\n",
    "        val = df_jump[column_to_plot].values\n",
    "        all_values.append(val)\n",
    "        if time_normalized is None:\n",
    "            time_normalized = df_jump['time_normalized'].values * 100 # In Prozent umrechnen\n",
    "            \n",
    "        axes[i].plot(time_normalized, val, color='black', lw=1)\n",
    "        axes[i].set_ylabel(\"Wert\", fontsize=8)\n",
    "        axes[i].set_title(f\"{key}\", fontsize=9, pad=2)\n",
    "        axes[i].grid(True, alpha=0.2)\n",
    "    \n",
    "    plt.xlabel(\"Zeitverlauf Normiert (0-100% vor Takeoff)\")\n",
    "    plt.tight_layout()\n",
    "    fig_sub.savefig(os.path.join(target_dir, f\"{participant_name}_{condition_type}_all_subplots.png\"))\n",
    "    plt.close(fig_sub)\n",
    "\n",
    "    # --- GRAFIK 2: √úBEREINANDERGELEGT (12 Spr√ºnge + 1 dicker Mittelwert) ---\n",
    "    plt.figure(figsize=(11, 7))\n",
    "    \n",
    "    data_matrix = np.array(all_values)\n",
    "    mean_val = np.mean(data_matrix, axis=0)\n",
    "    std_val = np.std(data_matrix, axis=0)\n",
    "\n",
    "    # Einzelne Spr√ºnge (sehr blass im Hintergrund)\n",
    "    for i, val in enumerate(all_values):\n",
    "        plt.plot(time_normalized, val, alpha=0.15, lw=1, color='gray')\n",
    "    \n",
    "    # Mittelwert (Fett in Farbe je nach Bedingung)\n",
    "    color = 'blue' if 'REAL' in condition_type else 'red'\n",
    "    plt.plot(time_normalized, mean_val, color=color, lw=4, label=f'Gesamt-Mittelwert {condition_type} (n={num_jumps})')\n",
    "    \n",
    "    # Standardabweichung (Schatten)\n",
    "    plt.fill_between(time_normalized, mean_val - std_val, mean_val + std_val, \n",
    "                     color=color, alpha=0.2, label=f'Standardabweichung (¬±1 SD)')\n",
    "    \n",
    "    plt.title(f\"Bedingungs-Analyse: {participant_name}\\nVariable: {column_to_plot} | Bedingung: {condition_type}\", fontsize=12)\n",
    "    plt.xlabel(\"Vorbereitungsphase in % (0% = Start, 100% = Takeoff)\")\n",
    "    plt.ylabel(column_to_plot)\n",
    "    plt.legend(loc='upper left', frameon=True)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Speichern Overlaid\n",
    "    plt.savefig(os.path.join(target_dir, f\"{participant_name}_{condition_type}_combined_mean.png\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    #print(f\"  ‚úì Kombinierte {condition_type}-Grafiken (n={num_jumps}) gespeichert in: {target_dir}\")\n",
    "\n",
    "### Plot funkton f√ºr einen Winkel\n",
    "def plot_everything_for_one_column_to_plot (normalized_dict, column_to_plot):\n",
    "    \"\"\"\n",
    "    Wrapper-Funktion, um f√ºr alle Teilnehmer und eine bestimmte Spalte die \n",
    "    kombinierten Plots zu erstellen.\n",
    "    \"\"\"\n",
    "    print(f\"\\nStarte Generierung der kombinierten Plots (12 Spr√ºnge pro Bedingung)...\")\n",
    "\n",
    "    for participant in participants:\n",
    "        #print(f\"\\nErstelle kombinierte Plots f√ºr {participant} - Variable: {column_to_plot}\")\n",
    "        \n",
    "        # Bedingungen REAL und VR\n",
    "        for condition in ['REAL', 'VR']:\n",
    "            plot_normalized_conditions(\n",
    "                normalized_dict[participant], \n",
    "                participant, \n",
    "                condition, \n",
    "                column_to_plot\n",
    "            )\n",
    "        \n",
    "        print (len(normalized_dict[participant]))\n",
    "        # print(f\"  ‚úì Bedingungen REAL und VR f√ºr {participant} erfolgreich zusammengefasst.\")\n",
    "    \n",
    "    print(f\"\\n{'='*30}\")\n",
    "    print(\"ALLE KOMBINIERTEN GRAFIKEN ERSTELLT\")\n",
    "    print(f\"Speicherort: Pictures_Test/Conditions_Combined/{column_to_plot.replace(' ', '_').replace('(', '').replace(')', '')}/\")\n",
    "    print(f\"{'='*30}\")  \n",
    "\n",
    "### globaler plot f√ºr alle Teilnehmer einer Bedingung\n",
    "def plot_global_condition_comparison(all_participants_dict, condition_type, column_to_plot, plot_true_false=False):\n",
    "    \"\"\"\n",
    "    Erstellt einen Plot f√ºr ALLE Teilnehmer zusammengefasst f√ºr eine Bedingung (REAL oder VR).\n",
    "    - D√ºnne graue Linien: Jeder einzelne Sprung (n=~84)\n",
    "    - Dicke farbige Linie: Gesamt-Mittelwert √ºber alle Teilnehmer\n",
    "    - Schattierter Bereich: Standardabweichung\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Vorbereitung Ordnerstruktur\n",
    "    target_dir = os.path.join('Pictures_Test', 'Global_Comparison', column_to_plot.replace(' ', '_'))\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    \n",
    "    all_values = []\n",
    "    time_normalized = None\n",
    "    total_jumps_count = 0\n",
    "\n",
    "    # 2. Daten sammeln √ºber alle Teilnehmer hinweg\n",
    "    for p_name, normalized_dict in all_participants_dict.items():\n",
    "        # Suche Spr√ºnge f√ºr die Bedingung (z.B. REAL) bei diesem Teilnehmer\n",
    "        jump_keys = [k for k in normalized_dict.keys() if condition_type in k]\n",
    "        \n",
    "        for key in jump_keys:\n",
    "            df_jump = normalized_dict[key]\n",
    "            val = df_jump[column_to_plot].values\n",
    "            all_values.append(val)\n",
    "            \n",
    "            if time_normalized is None:\n",
    "                # In Prozent umrechnen (0-100)\n",
    "                time_normalized = df_jump['time_normalized'].values * 100\n",
    "            \n",
    "            total_jumps_count += 1\n",
    "\n",
    "    if total_jumps_count == 0:\n",
    "        print(f\"Keine Daten f√ºr {condition_type} in der gesamten Gruppe gefunden.\")\n",
    "        return\n",
    "\n",
    "    # 3. Statistik berechnen\n",
    "    data_matrix = np.array(all_values)\n",
    "    mean_val = np.mean(data_matrix, axis=0)\n",
    "    std_val = np.std(data_matrix, axis=0)\n",
    "\n",
    "    # 4. Plotting\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Einzelne Spr√ºnge (84 d√ºnne Linien)\n",
    "    for val in all_values:\n",
    "        plt.plot(time_normalized, val, alpha=0.08, lw=0.8, color='gray') # Sehr blass f√ºr die Masse\n",
    "    \n",
    "    # Farbe festlegen\n",
    "    main_color = 'blue' if 'REAL' in condition_type else 'red'\n",
    "    \n",
    "    # Standardabweichung (Schatten)\n",
    "    plt.fill_between(time_normalized, mean_val - std_val, mean_val + std_val, \n",
    "                     color=main_color, alpha=0.15, label=f'SD (¬±1 œÉ)')\n",
    "    \n",
    "    # Mittelwert (Fett)\n",
    "    plt.plot(time_normalized, mean_val, color=main_color, lw=3.5, \n",
    "             label=f'Gesamt-Mittelwert {condition_type} (n={total_jumps_count} Spr√ºnge)')\n",
    "    \n",
    "    # Layout & Beschriftung\n",
    "    plt.title(f\"Globale Analyse: Alle Teilnehmer\\nVariable: {column_to_plot} | Bedingung: {condition_type}\", fontsize=14)\n",
    "    plt.xlabel(\"Bewegungszyklus in % (0% = Start, 100% = Takeoff)\", fontsize=12)\n",
    "    plt.ylabel(column_to_plot, fontsize=12)\n",
    "    plt.xlim(0, 100)\n",
    "    plt.legend(loc='best', frameon=True)\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Speichern\n",
    "    file_name = f\"GLOBAL_{condition_type}_{column_to_plot.replace(' ', '_')}.png\"\n",
    "    plt.savefig(os.path.join(target_dir, file_name), dpi=300, bbox_inches='tight')\n",
    "    if plot_true_false == True:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"  ‚úì Globaler Plot gespeichert: {file_name} (n={total_jumps_count} Spr√ºnge)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb7fb1d",
   "metadata": {},
   "source": [
    "### Gelenke pro Teilnehmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aaa1da9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starte Generierung der kombinierten Plots (12 Spr√ºnge pro Bedingung)...\n",
      "17\n",
      "14\n",
      "19\n",
      "17\n",
      "22\n",
      "17\n",
      "19\n",
      "\n",
      "==============================\n",
      "ALLE KOMBINIERTEN GRAFIKEN ERSTELLT\n",
      "Speicherort: Pictures_Test/Conditions_Combined/RT_Knee_Flexion_deg/\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# plots f√ºr Knie aufrufen\n",
    "colum_to_Plot = 'RT Knee Flexion (deg)'\n",
    "plot_everything_for_one_column_to_plot(all_normalized_data, colum_to_Plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03d62e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starte Generierung der kombinierten Plots (12 Spr√ºnge pro Bedingung)...\n",
      "17\n",
      "14\n",
      "19\n",
      "17\n",
      "22\n",
      "17\n",
      "19\n",
      "\n",
      "==============================\n",
      "ALLE KOMBINIERTEN GRAFIKEN ERSTELLT\n",
      "Speicherort: Pictures_Test/Conditions_Combined/RT_Hip_Flexion_deg/\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# plots f√ºr H√ºfte aufrufen\n",
    "colum_to_Plot = 'RT Hip Flexion (deg)'\n",
    "plot_everything_for_one_column_to_plot(all_normalized_data, colum_to_Plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b9cf180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starte Generierung der kombinierten Plots (12 Spr√ºnge pro Bedingung)...\n",
      "17\n",
      "14\n",
      "19\n",
      "17\n",
      "22\n",
      "17\n",
      "19\n",
      "\n",
      "==============================\n",
      "ALLE KOMBINIERTEN GRAFIKEN ERSTELLT\n",
      "Speicherort: Pictures_Test/Conditions_Combined/RT_Ankle_Dorsiflexion_deg/\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# plots f√ºr Sprunggelenk aufrufen\n",
    "colum_to_Plot = 'RT Ankle Dorsiflexion (deg)'\n",
    "plot_everything_for_one_column_to_plot(all_normalized_data, colum_to_Plot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27791d3a",
   "metadata": {},
   "source": [
    "### Globaler Plot mit allen Teilnehmern zusammen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a16222a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úì Globaler Plot gespeichert: GLOBAL_REAL_RT_Knee_Flexion_(deg).png (n=64 Spr√ºnge)\n",
      "  ‚úì Globaler Plot gespeichert: GLOBAL_VR_RT_Knee_Flexion_(deg).png (n=61 Spr√ºnge)\n",
      "  ‚úì Globaler Plot gespeichert: GLOBAL_REAL_RT_Hip_Flexion_(deg).png (n=64 Spr√ºnge)\n",
      "  ‚úì Globaler Plot gespeichert: GLOBAL_VR_RT_Hip_Flexion_(deg).png (n=61 Spr√ºnge)\n"
     ]
    }
   ],
   "source": [
    "### globale plots f√ºr Knie vergleichen\n",
    "colum_to_Plot = 'RT Knee Flexion (deg)'\n",
    "plot_global_condition_comparison(all_normalized_data, 'REAL', colum_to_Plot)\n",
    "plot_global_condition_comparison(all_normalized_data, 'VR', colum_to_Plot)\n",
    "\n",
    "### globale plots f√ºr H√ºfte vergleichen\n",
    "colum_to_Plot = 'RT Hip Flexion (deg)'\n",
    "plot_global_condition_comparison(all_normalized_data, 'REAL', colum_to_Plot)\n",
    "plot_global_condition_comparison(all_normalized_data, 'VR', colum_to_Plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaf586a",
   "metadata": {},
   "source": [
    "## Parameter Bestimmen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187f926e",
   "metadata": {},
   "source": [
    "### Maximaler Winkel innerhalb der Fenster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24ce7a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starte Extraktion der Maximalwerte...\n",
      "Verarbeite Parameter f√ºr ID_1_Dabisch_Samuel:\n",
      "  ‚úì Spalte 'RT_Knee_Max' erfolgreich aktualisiert.\n",
      "  ‚úì Spalte 'LT_Knee_Max' erfolgreich aktualisiert.\n",
      "  ‚úì Spalte 'RT_Hip_Max' erfolgreich aktualisiert.\n",
      "  ‚úì Spalte 'LT_Hip_Max' erfolgreich aktualisiert.\n",
      "  ‚úì Spalte 'RT_Ankle_Max' erfolgreich aktualisiert.\n",
      "  ‚úì Spalte 'LT_Ankle_Max' erfolgreich aktualisiert.\n",
      "Verarbeite Parameter f√ºr ID_2_Pohl_Jannis:\n",
      "  ‚úì Spalte 'RT_Knee_Max' erfolgreich aktualisiert.\n",
      "  ‚úì Spalte 'LT_Knee_Max' erfolgreich aktualisiert.\n",
      "  ‚úì Spalte 'RT_Hip_Max' erfolgreich aktualisiert.\n",
      "  ‚úì Spalte 'LT_Hip_Max' erfolgreich aktualisiert.\n",
      "  ‚úì Spalte 'RT_Ankle_Max' erfolgreich aktualisiert.\n",
      "  ‚úì Spalte 'LT_Ankle_Max' erfolgreich aktualisiert.\n",
      "Verarbeite Parameter f√ºr ID_3_Kleber_Christian:\n",
      "  ‚úì Spalte 'RT_Knee_Max' erfolgreich aktualisiert.\n",
      "  ‚úì Spalte 'LT_Knee_Max' erfolgreich aktualisiert.\n",
      "  ‚úì Spalte 'RT_Hip_Max' erfolgreich aktualisiert.\n",
      "  ‚úì Spalte 'LT_Hip_Max' erfolgreich aktualisiert.\n",
      "  ‚úì Spalte 'RT_Ankle_Max' erfolgreich aktualisiert.\n",
      "  ‚úì Spalte 'LT_Ankle_Max' erfolgreich aktualisiert.\n",
      "Verarbeite Parameter f√ºr ID_4_Schr√∂ter_Till:\n",
      "  ‚úì Spalte 'RT_Knee_Max' erfolgreich aktualisiert.\n",
      "  ‚úì Spalte 'LT_Knee_Max' erfolgreich aktualisiert.\n",
      "  ‚úì Spalte 'RT_Hip_Max' erfolgreich aktualisiert.\n",
      "  ‚úì Spalte 'LT_Hip_Max' erfolgreich aktualisiert.\n",
      "  ‚úì Spalte 'RT_Ankle_Max' erfolgreich aktualisiert.\n",
      "  ‚úì Spalte 'LT_Ankle_Max' erfolgreich aktualisiert.\n",
      "Verarbeite Parameter f√ºr ID_5_Zaschke_Lenard:\n",
      "  ‚úì Spalte 'RT_Knee_Max' erfolgreich aktualisiert.\n",
      "  ‚úì Spalte 'LT_Knee_Max' erfolgreich aktualisiert.\n",
      "  ‚úì Spalte 'RT_Hip_Max' erfolgreich aktualisiert.\n",
      "  ‚úì Spalte 'LT_Hip_Max' erfolgreich aktualisiert.\n",
      "  ‚úì Spalte 'RT_Ankle_Max' erfolgreich aktualisiert.\n",
      "  ‚úì Spalte 'LT_Ankle_Max' erfolgreich aktualisiert.\n",
      "Verarbeite Parameter f√ºr ID_6_Petroll_Finn:\n",
      "  ‚úì Spalte 'RT_Knee_Max' erfolgreich aktualisiert.\n",
      "  ‚úì Spalte 'LT_Knee_Max' erfolgreich aktualisiert.\n",
      "  ‚úì Spalte 'RT_Hip_Max' erfolgreich aktualisiert.\n",
      "  ‚úì Spalte 'LT_Hip_Max' erfolgreich aktualisiert.\n",
      "  ‚úì Spalte 'RT_Ankle_Max' erfolgreich aktualisiert.\n",
      "  ‚úì Spalte 'LT_Ankle_Max' erfolgreich aktualisiert.\n",
      "Verarbeite Parameter f√ºr ID_7_Gruber_Julius:\n",
      "  ‚úì Spalte 'RT_Knee_Max' erfolgreich aktualisiert.\n",
      "  ‚úì Spalte 'LT_Knee_Max' erfolgreich aktualisiert.\n",
      "  ‚úì Spalte 'RT_Hip_Max' erfolgreich aktualisiert.\n",
      "  ‚úì Spalte 'LT_Hip_Max' erfolgreich aktualisiert.\n",
      "  ‚úì Spalte 'RT_Ankle_Max' erfolgreich aktualisiert.\n",
      "  ‚úì Spalte 'LT_Ankle_Max' erfolgreich aktualisiert.\n",
      "\n",
      "==============================\n",
      "ALLE PARAMETER GESPEICHERT IM ORDNER 'Parameter_IMU'\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "### Parameter extrahieren\n",
    "def extract_max_parameter(normalized_dict, column_to_process):\n",
    "    \"\"\"\n",
    "    Extrahiert den Maximalwert einer Spalte f√ºr jeden Sprung.\n",
    "    Gibt ein Dictionary zur√ºck: {'REAL_1_jump_1': 65.4, ...}\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for jump_key, df_jump in normalized_dict.items():\n",
    "        if column_to_process in df_jump.columns:\n",
    "            max_val = df_jump[column_to_process].max()\n",
    "            results[jump_key] = round(max_val, 2)\n",
    "    \n",
    "    type = 'Maximalwert'\n",
    "    return results, type\n",
    "\n",
    "### Parameter speichern/aktualisieren\n",
    "def update_participant_parameters(participant_name, new_data_dict, column_label, type):\n",
    "    \"\"\"\n",
    "    Speichert Parameter in einer CSV. \n",
    "    Garantiert korrekte Spaltenbenennung und verhindert KeyErrors beim Mergen.\n",
    "    \"\"\"\n",
    "    folder = 'Parameter_IMU'\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    file_path = os.path.join(folder, f'{participant_name}_parameter_{type}.csv')\n",
    "    \n",
    "    # 1. Neuen DataFrame aus dem Dictionary erstellen\n",
    "    rows = []\n",
    "    for jump_key, value in new_data_dict.items():\n",
    "        if '_jump_' in jump_key:\n",
    "            parts = jump_key.split('_jump_')\n",
    "            trial = parts[0]\n",
    "            sprung_nr = int(parts[1]) # Als Zahl f√ºr saubere Sortierung\n",
    "        else:\n",
    "            trial, sprung_nr = jump_key, 0\n",
    "        \n",
    "        rows.append({\n",
    "            'participant': participant_name,\n",
    "            'trial': trial,\n",
    "            'sprung_nr': sprung_nr,\n",
    "            column_label: value\n",
    "        })\n",
    "    \n",
    "    new_df = pd.DataFrame(rows)\n",
    "    \n",
    "    # 2. Falls Datei existiert, einlesen und mergen\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            existing_df = pd.read_csv(file_path, sep=';')\n",
    "            \n",
    "            # Sicherheitscheck: Falls 'participant' fehlt, wurde die CSV falsch erstellt\n",
    "            if 'participant' not in existing_df.columns:\n",
    "                print(f\"  ! CSV von {participant_name} war fehlerhaft. Wird neu erstellt.\")\n",
    "                new_df.to_csv(file_path, sep=';', index=False)\n",
    "                return\n",
    "\n",
    "            # Falls die Spalte schon existiert, im alten DF l√∂schen (f√ºr das Update)\n",
    "            if column_label in existing_df.columns:\n",
    "                existing_df = existing_df.drop(columns=[column_label])\n",
    "            \n",
    "            # Mergen (Zusammenf√ºhren)\n",
    "            updated_df = pd.merge(\n",
    "                existing_df, \n",
    "                new_df, \n",
    "                on=['participant', 'trial', 'sprung_nr'], \n",
    "                how='outer'\n",
    "            )\n",
    "            \n",
    "            # Sortieren (Real_1 vor Real_2, Sprung 1 vor Sprung 2)\n",
    "            updated_df = updated_df.sort_values(by=['trial', 'sprung_nr'])\n",
    "            updated_df.to_csv(file_path, sep=';', index=False)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ! Fehler beim Update von {participant_name}: {e}. Erstelle Datei neu.\")\n",
    "            new_df.to_csv(file_path, sep=';', index=False)\n",
    "    else:\n",
    "        # 3. Datei existiert noch nicht: Neu erstellen\n",
    "        new_df.to_csv(file_path, sep=';', index=False)\n",
    "\n",
    "    print(f\"  ‚úì Spalte '{column_label}' erfolgreich aktualisiert.\")\n",
    "\n",
    "\n",
    "# %% Parameter-Extraktion (Maximalwerte) --------------------------------------\n",
    "print(f\"\\nStarte Extraktion der Maximalwerte...\")\n",
    "\n",
    "# Hier definierst du, welche Spalten du auswerten willst\n",
    "# Der Key ist die Spalte im DF, der Value ist der Name f√ºr deine CSV\n",
    "columns_to_analyze = {\n",
    "    'RT Knee Flexion (deg)': 'RT_Knee_Max',\n",
    "    'LT Knee Flexion (deg)': 'LT_Knee_Max',\n",
    "    'RT Hip Flexion (deg)': 'RT_Hip_Max',\n",
    "    'LT Hip Flexion (deg)': 'LT_Hip_Max',\n",
    "    'RT Ankle Dorsiflexion (deg)': 'RT_Ankle_Max',\n",
    "    'LT Ankle Dorsiflexion (deg)': 'LT_Ankle_Max'\n",
    "}\n",
    "\n",
    "for participant in participants:\n",
    "    print(f\"Verarbeite Parameter f√ºr {participant}:\")\n",
    "    \n",
    "    for raw_col, csv_label in columns_to_analyze.items():\n",
    "        # 1. Maxima berechnen\n",
    "        max_values, type  = extract_max_parameter(all_normalized_data[participant], raw_col)\n",
    "        \n",
    "        # 2. CSV updaten/erstellen\n",
    "        if max_values: # Nur wenn Daten gefunden wurden\n",
    "            update_participant_parameters(participant, max_values, csv_label, type)\n",
    "\n",
    "print(f\"\\n{'='*30}\")\n",
    "print(f\"ALLE PARAMETER GESPEICHERT IM ORDNER 'Parameter_IMU'\")\n",
    "print(f\"{'='*30}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7bf0dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Master-Tabelle mit 7 Teilnehmern erstellt.\n",
      "Datei gespeichert unter: Parameter_IMU/Master_Table_for_T-Test.csv\n",
      "\n",
      "Erstellte Spalten f√ºr den T-Test:\n",
      "['Participant_ID', 'RT_Knee_Max_REAL_Mean', 'RT_Knee_Max_VR_Mean', 'LT_Knee_Max_REAL_Mean', 'LT_Knee_Max_VR_Mean', 'RT_Hip_Max_REAL_Mean', 'RT_Hip_Max_VR_Mean', 'LT_Hip_Max_REAL_Mean', 'LT_Hip_Max_VR_Mean', 'RT_Ankle_Max_REAL_Mean', 'RT_Ankle_Max_VR_Mean', 'LT_Ankle_Max_REAL_Mean', 'LT_Ankle_Max_VR_Mean']\n"
     ]
    }
   ],
   "source": [
    "### CSV f√ºr t test erstellen\n",
    "def create_master_table_for_ttest(input_folder='Parameter_IMU'):\n",
    "    \"\"\"\n",
    "    Liest alle Teilnehmer-Parameter-CSVs und erstellt eine Master-Tabelle.\n",
    "    Pro Teilnehmer eine Zeile, Spalten getrennt nach REAL und VR Mittelwerten.\n",
    "    \"\"\"\n",
    "    # 1. Alle relevanten CSVs finden (wir suchen nach den 'Maximalwert' Dateien)\n",
    "    file_pattern = os.path.join(input_folder, '*_parameter_Maximalwert.csv')\n",
    "    files = glob.glob(file_pattern)\n",
    "    \n",
    "    if not files:\n",
    "        print(\"Keine Parameter-Dateien gefunden!\")\n",
    "        return\n",
    "\n",
    "    master_rows = []\n",
    "\n",
    "    for file_path in files:\n",
    "        # CSV einlesen (sep=';' wie in deinem Code definiert)\n",
    "        df = pd.read_csv(file_path, sep=';')\n",
    "        \n",
    "        participant_id = df['participant'].iloc[0]\n",
    "        \n",
    "        # Vorbereitung der Zeile f√ºr den Master-DF\n",
    "        row = {'Participant_ID': participant_id}\n",
    "        \n",
    "        # 2. Bedingungen trennen\n",
    "        # Wir suchen in der Spalte 'trial' nach 'REAL' oder 'VR'\n",
    "        df_real = df[df['trial'].str.contains('REAL', na=False)]\n",
    "        df_vr = df[df['trial'].str.contains('VR', na=False)]\n",
    "        \n",
    "        # 3. Spalten automatisch erkennen (alle au√üer Meta-Daten)\n",
    "        # Das sind: RT_Knee_Max, LT_Knee_Max, etc.\n",
    "        data_columns = [c for c in df.columns if c not in ['participant', 'trial', 'sprung_nr']]\n",
    "        \n",
    "        for col in data_columns:\n",
    "            # Mittelwert f√ºr REAL berechnen\n",
    "            if not df_real.empty:\n",
    "                row[f'{col}_REAL_Mean'] = round(df_real[col].mean(), 2)\n",
    "            else:\n",
    "                row[f'{col}_REAL_Mean'] = None\n",
    "                \n",
    "            # Mittelwert f√ºr VR berechnen\n",
    "            if not df_vr.empty:\n",
    "                row[f'{col}_VR_Mean'] = round(df_vr[col].mean(), 2)\n",
    "            else:\n",
    "                row[f'{col}_VR_Mean'] = None\n",
    "        \n",
    "        master_rows.append(row)\n",
    "\n",
    "    # 4. Master DataFrame erstellen\n",
    "    master_df = pd.DataFrame(master_rows)\n",
    "    \n",
    "    # Sortieren nach ID\n",
    "    master_df = master_df.sort_values('Participant_ID')\n",
    "    \n",
    "    # 5. Speichern\n",
    "    output_path = 'Parameter_IMU/Master_Table_for_T-Test.csv'\n",
    "    master_df.to_csv(output_path, sep=';', index=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Master-Tabelle mit {len(master_df)} Teilnehmern erstellt.\")\n",
    "    print(f\"Datei gespeichert unter: {output_path}\")\n",
    "    \n",
    "    return master_df\n",
    "\n",
    "# --- AUSF√úHRUNG ---\n",
    "master_data = create_master_table_for_ttest()\n",
    "\n",
    "# Kurze Vorschau der Spalten\n",
    "print(\"\\nErstellte Spalten f√ºr den T-Test:\")\n",
    "print(master_data.columns.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
